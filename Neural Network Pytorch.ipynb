{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xjtuyLsCYbNR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from torch.optim.lr_scheduler import CyclicLR\n",
        "import sklearn\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.model_selection import KFold\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader, random_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jM2jEtW4jl0S"
      },
      "outputs": [],
      "source": [
        "# Hyper-parameters \n",
        "input_size = 784 # 28x28\n",
        "hidden_size = 500 \n",
        "num_classes = 10\n",
        "num_epochs = 12\n",
        "batch_size = 96\n",
        "learning_rate = 0.001\n",
        "LR_DROP = 0.5\n",
        "LR_EPOCHS_DROP = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ad7WBPOKohDF"
      },
      "outputs": [],
      "source": [
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tWUJ4tCr1toL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e1daf49-4d22-498c-b76c-f7449d9aaa9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 101875035.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 27694488.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 26655045.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 13444268.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Load MNIST dataset\n",
        "dataset = datasets.MNIST(root='data/', train=True, download=True, transform=transforms.ToTensor())\n",
        "\n",
        "train = datasets.MNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
        "test = datasets.MNIST(root='./data', train=False, transform=transforms.ToTensor(), download=True)\n",
        "\n",
        "# Split dataset into train, validation, and test sets\n",
        "train_size = int(0.8 * len(train))\n",
        "val_size = int(0.2 * len(train))\n",
        "\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "# Create data loaders for each set\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Srdxv1cHOlad"
      },
      "outputs": [],
      "source": [
        "def lr_schedule(epoch, initial_lr, drop, epochs_drop):\n",
        "    learning_rate = initial_lr * (drop ** (epoch // epochs_drop))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Yue3qVEs4P6",
        "outputId": "bb75df3a-ea90-4ad6-b891-c227ca76af32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/9], Step [100/500], Loss: 0.2913\n",
            "Epoch [1/9], Step [200/500], Loss: 0.2715\n",
            "Epoch [1/9], Step [300/500], Loss: 0.1768\n",
            "Epoch [1/9], Step [400/500], Loss: 0.1611\n",
            "Epoch [1/9], Step [500/500], Loss: 0.1748\n",
            "Epoch [1/9], Train Loss: 0.3212, Val Loss: 0.1771, Train Acc: 91.13%, Val Acc: 94.77%\n",
            "Epoch [2/9], Step [100/500], Loss: 0.1025\n",
            "Epoch [2/9], Step [200/500], Loss: 0.1953\n",
            "Epoch [2/9], Step [300/500], Loss: 0.0960\n",
            "Epoch [2/9], Step [400/500], Loss: 0.2054\n",
            "Epoch [2/9], Step [500/500], Loss: 0.0780\n",
            "Epoch [2/9], Train Loss: 0.1291, Val Loss: 0.1274, Train Acc: 96.28%, Val Acc: 96.31%\n",
            "Epoch [3/9], Step [100/500], Loss: 0.0882\n",
            "Epoch [3/9], Step [200/500], Loss: 0.0404\n",
            "Epoch [3/9], Step [300/500], Loss: 0.0236\n",
            "Epoch [3/9], Step [400/500], Loss: 0.1003\n",
            "Epoch [3/9], Step [500/500], Loss: 0.1092\n",
            "Epoch [3/9], Train Loss: 0.0853, Val Loss: 0.1007, Train Acc: 97.51%, Val Acc: 96.79%\n",
            "Epoch [4/9], Step [100/500], Loss: 0.0246\n",
            "Epoch [4/9], Step [200/500], Loss: 0.0723\n",
            "Epoch [4/9], Step [300/500], Loss: 0.0423\n",
            "Epoch [4/9], Step [400/500], Loss: 0.0382\n",
            "Epoch [4/9], Step [500/500], Loss: 0.0895\n",
            "Epoch [4/9], Train Loss: 0.0610, Val Loss: 0.0937, Train Acc: 98.24%, Val Acc: 97.09%\n",
            "Epoch [5/9], Step [100/500], Loss: 0.0862\n",
            "Epoch [5/9], Step [200/500], Loss: 0.0132\n",
            "Epoch [5/9], Step [300/500], Loss: 0.0149\n",
            "Epoch [5/9], Step [400/500], Loss: 0.0549\n",
            "Epoch [5/9], Step [500/500], Loss: 0.0651\n",
            "Epoch [5/9], Train Loss: 0.0447, Val Loss: 0.0861, Train Acc: 98.69%, Val Acc: 97.38%\n",
            "Epoch [6/9], Step [100/500], Loss: 0.0803\n",
            "Epoch [6/9], Step [200/500], Loss: 0.0183\n",
            "Epoch [6/9], Step [300/500], Loss: 0.0205\n",
            "Epoch [6/9], Step [400/500], Loss: 0.0185\n",
            "Epoch [6/9], Step [500/500], Loss: 0.0440\n",
            "Epoch [6/9], Train Loss: 0.0332, Val Loss: 0.0804, Train Acc: 99.05%, Val Acc: 97.62%\n",
            "Epoch [7/9], Step [100/500], Loss: 0.0288\n",
            "Epoch [7/9], Step [200/500], Loss: 0.0036\n",
            "Epoch [7/9], Step [300/500], Loss: 0.0120\n",
            "Epoch [7/9], Step [400/500], Loss: 0.0439\n",
            "Epoch [7/9], Step [500/500], Loss: 0.0726\n",
            "Epoch [7/9], Train Loss: 0.0262, Val Loss: 0.0873, Train Acc: 99.23%, Val Acc: 97.41%\n",
            "Epoch [8/9], Step [100/500], Loss: 0.0254\n",
            "Epoch [8/9], Step [200/500], Loss: 0.0131\n",
            "Epoch [8/9], Step [300/500], Loss: 0.0341\n",
            "Epoch [8/9], Step [400/500], Loss: 0.0303\n",
            "Epoch [8/9], Step [500/500], Loss: 0.0163\n",
            "Epoch [8/9], Train Loss: 0.0186, Val Loss: 0.0818, Train Acc: 99.50%, Val Acc: 97.61%\n",
            "Epoch [9/9], Step [100/500], Loss: 0.0070\n",
            "Epoch [9/9], Step [200/500], Loss: 0.0492\n",
            "Epoch [9/9], Step [300/500], Loss: 0.0135\n",
            "Epoch [9/9], Step [400/500], Loss: 0.0479\n",
            "Epoch [9/9], Step [500/500], Loss: 0.0043\n",
            "Epoch [9/9], Train Loss: 0.0153, Val Loss: 0.0835, Train Acc: 99.60%, Val Acc: 97.67%\n"
          ]
        }
      ],
      "source": [
        "# with a custom learning rate function\n",
        "\n",
        "num_epochs = 9\n",
        "\n",
        "class NeuralNetworkRelu(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, num_classes):\n",
        "    super(NeuralNetworkRelu, self).__init__()\n",
        "    self.l1 = nn.Linear(input_size, hidden_size) \n",
        "    self.relu = nn.ReLU()\n",
        "    self.l2 = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.l1(x)\n",
        "    out = self.relu(out)\n",
        "    out = self.l2(out)\n",
        "    return out\n",
        "    \n",
        "\n",
        "modelRelu = NeuralNetworkRelu(input_size, hidden_size, num_classes).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(modelRelu.parameters(), lr=learning_rate)\n",
        "\n",
        "# Track the training and validation loss and accuracy for each epoch\n",
        "train_loss_history = []\n",
        "val_loss_history = []\n",
        "train_acc_history = []\n",
        "val_acc_history = []\n",
        "\n",
        "n_total_steps = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = 0.0\n",
        "    train_correct = 0\n",
        "    train_total = 0\n",
        "    for i, (images, labels) in enumerate(train_loader):  \n",
        "        # origin shape: [100, 1, 28, 28]\n",
        "        # resized: [100, 784]\n",
        "        images = images.reshape(-1, 28*28).to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        # Forward pass and loss calculation\n",
        "        outputs = modelRelu(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Backward and optimize\n",
        "        loss.backward()\n",
        "        # clip gradients \n",
        "        torch.nn.utils.clip_grad_norm_(modelRelu.parameters(), 5)\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        lr_schedule(epoch, learning_rate, LR_DROP, LR_EPOCHS_DROP)\n",
        "\n",
        "        # Update the training loss and accuracy\n",
        "        train_loss += loss.item() * images.size(0)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        train_correct += (predicted == labels).sum().item()\n",
        "        train_total += labels.size(0)\n",
        "                \n",
        "        if (i+1) % 100 == 0:\n",
        "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
        "\n",
        "    # Evaluate the model on the validation set\n",
        "    val_loss = 0.0\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images = images.reshape(-1, 28*28).to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = modelRelu(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            val_correct += (predicted == labels).sum().item()\n",
        "            val_total += labels.size(0)\n",
        "\n",
        "    # Compute the average training and validation loss and accuracy for the epoch\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "    val_loss /= len(val_loader.dataset)\n",
        "    \n",
        "    if train_total > 0:\n",
        "        train_acc = 100 * train_correct / train_total\n",
        "    else:\n",
        "        train_acc = 0\n",
        "\n",
        "    val_acc = 100 * val_correct / val_total\n",
        "\n",
        "    # Print the training and validation loss and accuracy for the epoch\n",
        "    print (f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%')\n",
        "\n",
        "    # Record the training and validation loss and accuracy for the epoch\n",
        "    train_loss_history.append(train_loss)\n",
        "    val_loss_history.append"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FGKNNc3JwaUp",
        "outputId": "bf8c5662-8f5e-4892-bfe6-5f9de34a0fe6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the 10000 test images: 97.85000000000001 %\n"
          ]
        }
      ],
      "source": [
        "# Test the model: we don't need to compute gradients\n",
        "with torch.no_grad():\n",
        "    n_correct = 0\n",
        "    n_examples = len(test_loader.dataset)\n",
        "\n",
        "    for images, labels in test_loader:\n",
        "        images = images.reshape(-1, 28*28).to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = modelRelu(images)\n",
        "\n",
        "        # max returns (output_value ,index)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        n_correct += (predicted == labels).sum().item()\n",
        "\n",
        "    acc = n_correct / n_examples\n",
        "    print(f'Accuracy of the network on the {n_examples} test images: {100*acc} %')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOCpgpJWfbt5",
        "outputId": "8408750b-c685-4e28-c641-36139276503a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/9], Step [100/500], Loss: 0.3348\n",
            "Epoch [1/9], Step [200/500], Loss: 0.2457\n",
            "Epoch [1/9], Step [300/500], Loss: 0.3087\n",
            "Epoch [1/9], Step [400/500], Loss: 0.1656\n",
            "Epoch [1/9], Step [500/500], Loss: 0.2191\n",
            "Epoch [1/9], Train Loss: 0.3924, Val Loss: 0.2435, Train Acc: 89.68%, Val Acc: 93.10%\n",
            "Epoch [2/9], Step [100/500], Loss: 0.3000\n",
            "Epoch [2/9], Step [200/500], Loss: 0.1059\n",
            "Epoch [2/9], Step [300/500], Loss: 0.1673\n",
            "Epoch [2/9], Step [400/500], Loss: 0.2012\n",
            "Epoch [2/9], Step [500/500], Loss: 0.2507\n",
            "Epoch [2/9], Train Loss: 0.1782, Val Loss: 0.1662, Train Acc: 94.92%, Val Acc: 94.98%\n",
            "Epoch [3/9], Step [100/500], Loss: 0.2204\n",
            "Epoch [3/9], Step [200/500], Loss: 0.1674\n",
            "Epoch [3/9], Step [300/500], Loss: 0.1229\n",
            "Epoch [3/9], Step [400/500], Loss: 0.1656\n",
            "Epoch [3/9], Step [500/500], Loss: 0.1196\n",
            "Epoch [3/9], Train Loss: 0.1251, Val Loss: 0.1292, Train Acc: 96.37%, Val Acc: 96.05%\n",
            "Epoch [4/9], Step [100/500], Loss: 0.1151\n",
            "Epoch [4/9], Step [200/500], Loss: 0.0640\n",
            "Epoch [4/9], Step [300/500], Loss: 0.1788\n",
            "Epoch [4/9], Step [400/500], Loss: 0.0998\n",
            "Epoch [4/9], Step [500/500], Loss: 0.1172\n",
            "Epoch [4/9], Train Loss: 0.0942, Val Loss: 0.1214, Train Acc: 97.31%, Val Acc: 96.30%\n",
            "Epoch [5/9], Step [100/500], Loss: 0.0745\n",
            "Epoch [5/9], Step [200/500], Loss: 0.1960\n",
            "Epoch [5/9], Step [300/500], Loss: 0.0398\n",
            "Epoch [5/9], Step [400/500], Loss: 0.0567\n",
            "Epoch [5/9], Step [500/500], Loss: 0.0605\n",
            "Epoch [5/9], Train Loss: 0.0734, Val Loss: 0.1037, Train Acc: 97.91%, Val Acc: 96.91%\n",
            "Epoch [6/9], Step [100/500], Loss: 0.1404\n",
            "Epoch [6/9], Step [200/500], Loss: 0.0746\n",
            "Epoch [6/9], Step [300/500], Loss: 0.0344\n",
            "Epoch [6/9], Step [400/500], Loss: 0.0440\n",
            "Epoch [6/9], Step [500/500], Loss: 0.0446\n",
            "Epoch [6/9], Train Loss: 0.0592, Val Loss: 0.0922, Train Acc: 98.26%, Val Acc: 97.17%\n",
            "Epoch [7/9], Step [100/500], Loss: 0.0234\n",
            "Epoch [7/9], Step [200/500], Loss: 0.0985\n",
            "Epoch [7/9], Step [300/500], Loss: 0.0924\n",
            "Epoch [7/9], Step [400/500], Loss: 0.0343\n",
            "Epoch [7/9], Step [500/500], Loss: 0.1126\n",
            "Epoch [7/9], Train Loss: 0.0472, Val Loss: 0.0864, Train Acc: 98.69%, Val Acc: 97.50%\n",
            "Epoch [8/9], Step [100/500], Loss: 0.0231\n",
            "Epoch [8/9], Step [200/500], Loss: 0.0306\n",
            "Epoch [8/9], Step [300/500], Loss: 0.0460\n",
            "Epoch [8/9], Step [400/500], Loss: 0.0322\n",
            "Epoch [8/9], Step [500/500], Loss: 0.0236\n",
            "Epoch [8/9], Train Loss: 0.0390, Val Loss: 0.0861, Train Acc: 98.92%, Val Acc: 97.40%\n",
            "Epoch [9/9], Step [100/500], Loss: 0.0300\n",
            "Epoch [9/9], Step [200/500], Loss: 0.0153\n",
            "Epoch [9/9], Step [300/500], Loss: 0.0220\n",
            "Epoch [9/9], Step [400/500], Loss: 0.0802\n",
            "Epoch [9/9], Step [500/500], Loss: 0.0778\n",
            "Epoch [9/9], Train Loss: 0.0326, Val Loss: 0.0877, Train Acc: 99.14%, Val Acc: 97.34%\n"
          ]
        }
      ],
      "source": [
        "# With cosine annealing learning rate function\n",
        "\n",
        "num_epochs = 9\n",
        "\n",
        "class NeuralNetworkRelu(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, num_classes):\n",
        "    super(NeuralNetworkRelu, self).__init__()\n",
        "    self.l1 = nn.Linear(input_size, hidden_size) \n",
        "    self.relu = nn.ReLU()\n",
        "    self.l2 = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.l1(x)\n",
        "    out = self.relu(out)\n",
        "    out = self.l2(out)\n",
        "    return out\n",
        "\n",
        "modelRelu = NeuralNetworkRelu(input_size, hidden_size, num_classes).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(modelRelu.parameters(), lr=learning_rate)\n",
        "scheduler = CosineAnnealingLR(optimizer,\n",
        "                              T_max = 32, # Maximum number of iterations.\n",
        "                             eta_min = 1e-4) # Minimum learning rate.\n",
        "\n",
        "# Track the training and validation loss and accuracy for each epoch\n",
        "train_loss_history = []\n",
        "val_loss_history = []\n",
        "train_acc_history = []\n",
        "val_acc_history = []\n",
        "\n",
        "n_total_steps = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = 0.0\n",
        "    train_correct = 0\n",
        "    train_total = 0\n",
        "    for i, (images, labels) in enumerate(train_loader):  \n",
        "        # origin shape: [100, 1, 28, 28]\n",
        "        # resized: [100, 784]\n",
        "        images = images.reshape(-1, 28*28).to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        # Forward pass and loss calculation\n",
        "        outputs = modelRelu(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Backward and optimize\n",
        "        loss.backward()\n",
        "        # clip gradients \n",
        "        torch.nn.utils.clip_grad_norm_(modelRelu.parameters(), 5)\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        scheduler.step()\n",
        "\n",
        "        # Update the training loss and accuracy\n",
        "        train_loss += loss.item() * images.size(0)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        train_correct += (predicted == labels).sum().item()\n",
        "        train_total += labels.size(0)\n",
        "        \n",
        "        if (i+1) % 100 == 0:\n",
        "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
        "\n",
        "    # Evaluate the model on the validation set\n",
        "    val_loss = 0.0\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images = images.reshape(-1, 28*28).to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = modelRelu(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            val_correct += (predicted == labels).sum().item()\n",
        "            val_total += labels.size(0)\n",
        "\n",
        "    # Compute the average training and validation loss and accuracy for the epoch\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "    val_loss /= len(val_loader.dataset)\n",
        "    \n",
        "    if train_total > 0:\n",
        "        train_acc = 100 * train_correct / train_total\n",
        "    else:\n",
        "        train_acc = 0\n",
        "\n",
        "    val_acc = 100 * val_correct / val_total\n",
        "\n",
        "    # Print the training and validation loss and accuracy for the epoch\n",
        "    print (f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%')\n",
        "\n",
        "    # Record the training and validation loss and accuracy for the epoch\n",
        "    train_loss_history.append(train_loss)\n",
        "    val_loss_history.append"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_PsTb8jxdBfX",
        "outputId": "4bf0d559-b3c6-430f-e5dc-23bf2b6bfca7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the 10000 test images: 97.61 %\n"
          ]
        }
      ],
      "source": [
        "# Test the model: we don't need to compute gradients\n",
        "with torch.no_grad():\n",
        "    n_correct = 0\n",
        "    n_examples = len(test_loader.dataset)\n",
        "\n",
        "    for images, labels in test_loader:\n",
        "        images = images.reshape(-1, 28*28).to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = modelRelu(images)\n",
        "\n",
        "        # max returns (output_value ,index)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        n_correct += (predicted == labels).sum().item()\n",
        "\n",
        "    acc = n_correct / n_examples\n",
        "    print(f'Accuracy of the network on the {n_examples} test images: {100*acc} %')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "APHVy0WKP4-m",
        "outputId": "a8ac9a60-7d34-4b0a-c72b-701e9eb2a1b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/12], Step [100/500], Loss: 0.3540\n",
            "Epoch [1/12], Step [200/500], Loss: 0.3389\n",
            "Epoch [1/12], Step [300/500], Loss: 0.1977\n",
            "Epoch [1/12], Step [400/500], Loss: 0.0890\n",
            "Epoch [1/12], Step [500/500], Loss: 0.1945\n",
            "Epoch [1/12], Train Loss: 0.2976, Val Loss: 0.1533, Train Acc: 91.29%, Val Acc: 95.49%\n",
            "Epoch [2/12], Step [100/500], Loss: 0.1413\n",
            "Epoch [2/12], Step [200/500], Loss: 0.1078\n",
            "Epoch [2/12], Step [300/500], Loss: 0.0688\n",
            "Epoch [2/12], Step [400/500], Loss: 0.0767\n",
            "Epoch [2/12], Step [500/500], Loss: 0.1204\n",
            "Epoch [2/12], Train Loss: 0.1119, Val Loss: 0.1140, Train Acc: 96.72%, Val Acc: 96.78%\n",
            "Epoch [3/12], Step [100/500], Loss: 0.1300\n",
            "Epoch [3/12], Step [200/500], Loss: 0.0213\n",
            "Epoch [3/12], Step [300/500], Loss: 0.0340\n",
            "Epoch [3/12], Step [400/500], Loss: 0.0629\n",
            "Epoch [3/12], Step [500/500], Loss: 0.0508\n",
            "Epoch [3/12], Train Loss: 0.0734, Val Loss: 0.0990, Train Acc: 97.86%, Val Acc: 96.97%\n",
            "Epoch [4/12], Step [100/500], Loss: 0.0218\n",
            "Epoch [4/12], Step [200/500], Loss: 0.1129\n",
            "Epoch [4/12], Step [300/500], Loss: 0.0295\n",
            "Epoch [4/12], Step [400/500], Loss: 0.0371\n",
            "Epoch [4/12], Step [500/500], Loss: 0.0427\n",
            "Epoch [4/12], Train Loss: 0.0532, Val Loss: 0.0903, Train Acc: 98.42%, Val Acc: 97.35%\n",
            "Epoch [5/12], Step [100/500], Loss: 0.0134\n",
            "Epoch [5/12], Step [200/500], Loss: 0.0670\n",
            "Epoch [5/12], Step [300/500], Loss: 0.0536\n",
            "Epoch [5/12], Step [400/500], Loss: 0.0387\n",
            "Epoch [5/12], Step [500/500], Loss: 0.0727\n",
            "Epoch [5/12], Train Loss: 0.0396, Val Loss: 0.0889, Train Acc: 98.84%, Val Acc: 97.36%\n",
            "Epoch [6/12], Step [100/500], Loss: 0.0142\n",
            "Epoch [6/12], Step [200/500], Loss: 0.0179\n",
            "Epoch [6/12], Step [300/500], Loss: 0.0210\n",
            "Epoch [6/12], Step [400/500], Loss: 0.0258\n",
            "Epoch [6/12], Step [500/500], Loss: 0.0238\n",
            "Epoch [6/12], Train Loss: 0.0311, Val Loss: 0.0813, Train Acc: 99.06%, Val Acc: 97.67%\n",
            "Epoch [7/12], Step [100/500], Loss: 0.0271\n",
            "Epoch [7/12], Step [200/500], Loss: 0.0502\n",
            "Epoch [7/12], Step [300/500], Loss: 0.0367\n",
            "Epoch [7/12], Step [400/500], Loss: 0.0291\n",
            "Epoch [7/12], Step [500/500], Loss: 0.0967\n",
            "Epoch [7/12], Train Loss: 0.0233, Val Loss: 0.0771, Train Acc: 99.38%, Val Acc: 97.86%\n",
            "Epoch [8/12], Step [100/500], Loss: 0.0069\n",
            "Epoch [8/12], Step [200/500], Loss: 0.0256\n",
            "Epoch [8/12], Step [300/500], Loss: 0.0115\n",
            "Epoch [8/12], Step [400/500], Loss: 0.0126\n",
            "Epoch [8/12], Step [500/500], Loss: 0.0068\n",
            "Epoch [8/12], Train Loss: 0.0168, Val Loss: 0.0752, Train Acc: 99.59%, Val Acc: 97.78%\n",
            "Epoch [9/12], Step [100/500], Loss: 0.0091\n",
            "Epoch [9/12], Step [200/500], Loss: 0.0012\n",
            "Epoch [9/12], Step [300/500], Loss: 0.0100\n",
            "Epoch [9/12], Step [400/500], Loss: 0.0144\n",
            "Epoch [9/12], Step [500/500], Loss: 0.0049\n",
            "Epoch [9/12], Train Loss: 0.0122, Val Loss: 0.0754, Train Acc: 99.75%, Val Acc: 97.90%\n",
            "Epoch [10/12], Step [100/500], Loss: 0.0046\n",
            "Epoch [10/12], Step [200/500], Loss: 0.0055\n",
            "Epoch [10/12], Step [300/500], Loss: 0.0046\n",
            "Epoch [10/12], Step [400/500], Loss: 0.0109\n",
            "Epoch [10/12], Step [500/500], Loss: 0.0149\n",
            "Epoch [10/12], Train Loss: 0.0092, Val Loss: 0.0794, Train Acc: 99.83%, Val Acc: 97.80%\n",
            "Epoch [11/12], Step [100/500], Loss: 0.0026\n",
            "Epoch [11/12], Step [200/500], Loss: 0.0064\n",
            "Epoch [11/12], Step [300/500], Loss: 0.0036\n",
            "Epoch [11/12], Step [400/500], Loss: 0.0185\n",
            "Epoch [11/12], Step [500/500], Loss: 0.0042\n",
            "Epoch [11/12], Train Loss: 0.0064, Val Loss: 0.0754, Train Acc: 99.92%, Val Acc: 97.96%\n",
            "Epoch [12/12], Step [100/500], Loss: 0.0024\n",
            "Epoch [12/12], Step [200/500], Loss: 0.0105\n",
            "Epoch [12/12], Step [300/500], Loss: 0.0056\n",
            "Epoch [12/12], Step [400/500], Loss: 0.0009\n",
            "Epoch [12/12], Step [500/500], Loss: 0.0049\n",
            "Epoch [12/12], Train Loss: 0.0048, Val Loss: 0.0753, Train Acc: 99.96%, Val Acc: 97.93%\n"
          ]
        }
      ],
      "source": [
        "# With CyclicLR learning rate function\n",
        "\n",
        "num_epochs = 12\n",
        "learning_rate = 0.1\n",
        "\n",
        "class NeuralNetworkRelu(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, num_classes):\n",
        "    super(NeuralNetworkRelu, self).__init__()\n",
        "    self.l1 = nn.Linear(input_size, hidden_size) \n",
        "    self.relu = nn.ReLU()\n",
        "    self.l2 = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.l1(x)\n",
        "    out = self.relu(out)\n",
        "    out = self.l2(out)\n",
        "    return out\n",
        "\n",
        "modelRelu = NeuralNetworkRelu(input_size, hidden_size, num_classes).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "#Stochastic gradient descend optimizer\n",
        "optimizer = torch.optim.SGD(modelRelu.parameters(), lr=learning_rate, momentum=0.9)\n",
        "scheduler = CyclicLR(optimizer, \n",
        "                     base_lr = learning_rate, # Initial learning rate which is the lower boundary in the cycle for each parameter group\n",
        "                     max_lr = 1e-1, # Upper learning rate boundaries in the cycle for each parameter group\n",
        "                     step_size_up = 4, # Number of training iterations in the increasing half of a cycle\n",
        "                     mode = \"triangular\")\n",
        "\n",
        "# Track the training and validation loss and accuracy for each epoch\n",
        "train_loss_history = []\n",
        "val_loss_history = []\n",
        "train_acc_history = []\n",
        "val_acc_history = []\n",
        "\n",
        "n_total_steps = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = 0.0\n",
        "    train_correct = 0\n",
        "    train_total = 0\n",
        "    for i, (images, labels) in enumerate(train_loader):  \n",
        "        # origin shape: [100, 1, 28, 28]\n",
        "        # resized: [100, 784]\n",
        "        images = images.reshape(-1, 28*28).to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        # Forward pass and loss calculation\n",
        "        outputs = modelRelu(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Backward and optimize\n",
        "        loss.backward()\n",
        "        # clip gradients \n",
        "        torch.nn.utils.clip_grad_norm_(modelRelu.parameters(), 5)\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        scheduler.step()\n",
        "\n",
        "        # Update the training loss and accuracy\n",
        "        train_loss += loss.item() * images.size(0)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        train_correct += (predicted == labels).sum().item()\n",
        "        train_total += labels.size(0)\n",
        "\n",
        "        if (i+1) % 100 == 0:\n",
        "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
        "\n",
        "    # Evaluate the model on the validation set\n",
        "    val_loss = 0.0\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images = images.reshape(-1, 28*28).to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = modelRelu(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            val_correct += (predicted == labels).sum().item()\n",
        "            val_total += labels.size(0)\n",
        "\n",
        "    # Compute the average training and validation loss and accuracy for the epoch\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "    val_loss /= len(val_loader.dataset)\n",
        "    \n",
        "    if train_total > 0:\n",
        "        train_acc = 100 * train_correct / train_total\n",
        "    else:\n",
        "        train_acc = 0\n",
        "\n",
        "    val_acc = 100 * val_correct / val_total\n",
        "\n",
        "    # Print the training and validation loss and accuracy for the epoch\n",
        "    print (f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%')\n",
        "\n",
        "    # Record the training and validation loss and accuracy for the epoch\n",
        "    train_loss_history.append(train_loss)\n",
        "    val_loss_history.append"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymbiCZN7JXcj",
        "outputId": "4eb64379-56c9-452d-ced2-858a4060e168"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the 10000 test images: 98.19 %\n"
          ]
        }
      ],
      "source": [
        "# Test the model: we don't need to compute gradients\n",
        "with torch.no_grad():\n",
        "    n_correct = 0\n",
        "    n_examples = len(test_loader.dataset)\n",
        "\n",
        "    for images, labels in test_loader:\n",
        "        images = images.reshape(-1, 28*28).to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = modelRelu(images)\n",
        "\n",
        "        # max returns (output_value ,index)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        n_correct += (predicted == labels).sum().item()\n",
        "\n",
        "    acc = n_correct / n_examples\n",
        "    print(f'Accuracy of the network on the {n_examples} test images: {100*acc} %')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPkKYF3aP3HV",
        "outputId": "bb70da03-bec4-4ec5-eba3-e7def08568b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/12], Step [100/500], Loss: 0.6597\n",
            "Epoch [1/12], Step [200/500], Loss: 0.4014\n",
            "Epoch [1/12], Step [300/500], Loss: 0.3262\n",
            "Epoch [1/12], Step [400/500], Loss: 0.3082\n",
            "Epoch [1/12], Step [500/500], Loss: 0.1448\n",
            "Epoch [1/12], Train Loss: 0.5217, Val Loss: 0.2950, Train Acc: 86.46%, Val Acc: 91.41%\n",
            "Epoch [2/12], Step [100/500], Loss: 0.2304\n",
            "Epoch [2/12], Step [200/500], Loss: 0.2254\n",
            "Epoch [2/12], Step [300/500], Loss: 0.2599\n",
            "Epoch [2/12], Step [400/500], Loss: 0.1437\n",
            "Epoch [2/12], Step [500/500], Loss: 0.1048\n",
            "Epoch [2/12], Train Loss: 0.2420, Val Loss: 0.2404, Train Acc: 93.01%, Val Acc: 92.78%\n",
            "Epoch [3/12], Step [100/500], Loss: 0.2670\n",
            "Epoch [3/12], Step [200/500], Loss: 0.1831\n",
            "Epoch [3/12], Step [300/500], Loss: 0.1892\n",
            "Epoch [3/12], Step [400/500], Loss: 0.2039\n",
            "Epoch [3/12], Step [500/500], Loss: 0.0958\n",
            "Epoch [3/12], Train Loss: 0.1857, Val Loss: 0.1941, Train Acc: 94.68%, Val Acc: 94.28%\n",
            "Epoch [4/12], Step [100/500], Loss: 0.0855\n",
            "Epoch [4/12], Step [200/500], Loss: 0.1606\n",
            "Epoch [4/12], Step [300/500], Loss: 0.3068\n",
            "Epoch [4/12], Step [400/500], Loss: 0.3325\n",
            "Epoch [4/12], Step [500/500], Loss: 0.1110\n",
            "Epoch [4/12], Train Loss: 0.1480, Val Loss: 0.1629, Train Acc: 95.69%, Val Acc: 95.10%\n",
            "Epoch [5/12], Step [100/500], Loss: 0.1654\n",
            "Epoch [5/12], Step [200/500], Loss: 0.1434\n",
            "Epoch [5/12], Step [300/500], Loss: 0.1295\n",
            "Epoch [5/12], Step [400/500], Loss: 0.0712\n",
            "Epoch [5/12], Step [500/500], Loss: 0.1659\n",
            "Epoch [5/12], Train Loss: 0.1204, Val Loss: 0.1450, Train Acc: 96.49%, Val Acc: 95.56%\n",
            "Epoch [6/12], Step [100/500], Loss: 0.1053\n",
            "Epoch [6/12], Step [200/500], Loss: 0.1198\n",
            "Epoch [6/12], Step [300/500], Loss: 0.0921\n",
            "Epoch [6/12], Step [400/500], Loss: 0.1180\n",
            "Epoch [6/12], Step [500/500], Loss: 0.1264\n",
            "Epoch [6/12], Train Loss: 0.0996, Val Loss: 0.1276, Train Acc: 97.10%, Val Acc: 96.25%\n",
            "Epoch [7/12], Step [100/500], Loss: 0.0614\n",
            "Epoch [7/12], Step [200/500], Loss: 0.0686\n",
            "Epoch [7/12], Step [300/500], Loss: 0.0558\n",
            "Epoch [7/12], Step [400/500], Loss: 0.1181\n",
            "Epoch [7/12], Step [500/500], Loss: 0.1454\n",
            "Epoch [7/12], Train Loss: 0.0826, Val Loss: 0.1134, Train Acc: 97.64%, Val Acc: 96.58%\n",
            "Epoch [8/12], Step [100/500], Loss: 0.0201\n",
            "Epoch [8/12], Step [200/500], Loss: 0.0614\n",
            "Epoch [8/12], Step [300/500], Loss: 0.0326\n",
            "Epoch [8/12], Step [400/500], Loss: 0.1008\n",
            "Epoch [8/12], Step [500/500], Loss: 0.0950\n",
            "Epoch [8/12], Train Loss: 0.0691, Val Loss: 0.1072, Train Acc: 98.05%, Val Acc: 96.66%\n",
            "Epoch [9/12], Step [100/500], Loss: 0.0184\n",
            "Epoch [9/12], Step [200/500], Loss: 0.0575\n",
            "Epoch [9/12], Step [300/500], Loss: 0.0775\n",
            "Epoch [9/12], Step [400/500], Loss: 0.0335\n",
            "Epoch [9/12], Step [500/500], Loss: 0.0529\n",
            "Epoch [9/12], Train Loss: 0.0585, Val Loss: 0.1046, Train Acc: 98.33%, Val Acc: 96.81%\n",
            "Epoch [10/12], Step [100/500], Loss: 0.0448\n",
            "Epoch [10/12], Step [200/500], Loss: 0.0248\n",
            "Epoch [10/12], Step [300/500], Loss: 0.0580\n",
            "Epoch [10/12], Step [400/500], Loss: 0.0666\n",
            "Epoch [10/12], Step [500/500], Loss: 0.0602\n",
            "Epoch [10/12], Train Loss: 0.0491, Val Loss: 0.0968, Train Acc: 98.62%, Val Acc: 97.09%\n",
            "Epoch [11/12], Step [100/500], Loss: 0.0824\n",
            "Epoch [11/12], Step [200/500], Loss: 0.0089\n",
            "Epoch [11/12], Step [300/500], Loss: 0.0167\n",
            "Epoch [11/12], Step [400/500], Loss: 0.0578\n",
            "Epoch [11/12], Step [500/500], Loss: 0.0247\n",
            "Epoch [11/12], Train Loss: 0.0405, Val Loss: 0.0923, Train Acc: 98.88%, Val Acc: 97.21%\n",
            "Epoch [12/12], Step [100/500], Loss: 0.0092\n",
            "Epoch [12/12], Step [200/500], Loss: 0.0252\n",
            "Epoch [12/12], Step [300/500], Loss: 0.0179\n",
            "Epoch [12/12], Step [400/500], Loss: 0.0618\n",
            "Epoch [12/12], Step [500/500], Loss: 0.0162\n",
            "Epoch [12/12], Train Loss: 0.0339, Val Loss: 0.0884, Train Acc: 99.12%, Val Acc: 97.37%\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 12\n",
        "learning_rate = 0.001\n",
        "\n",
        "class NeuralNetworkSigmoid(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, num_classes):\n",
        "    super(NeuralNetworkSigmoid, self).__init__()\n",
        "    self.l1 = nn.Linear(input_size, hidden_size) \n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "    self.l2 = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.l1(x)\n",
        "    out = self.sigmoid(out)\n",
        "    out = self.l2(out)\n",
        "    return out\n",
        "\n",
        "modelSigmoid = NeuralNetworkSigmoid(input_size, hidden_size, num_classes).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(modelSigmoid.parameters(), lr=learning_rate)\n",
        "\n",
        "# Track the training and validation loss and accuracy for each epoch\n",
        "train_loss_history = []\n",
        "val_loss_history = []\n",
        "train_acc_history = []\n",
        "val_acc_history = []\n",
        "\n",
        "n_total_steps = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = 0.0\n",
        "    train_correct = 0\n",
        "    train_total = 0\n",
        "    for i, (images, labels) in enumerate(train_loader):  \n",
        "        # origin shape: [100, 1, 28, 28]\n",
        "        # resized: [100, 784]\n",
        "        images = images.reshape(-1, 28*28).to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        # Forward pass and loss calculation\n",
        "        outputs = modelSigmoid(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Backward and optimize\n",
        "        loss.backward()\n",
        "        # clip gradients \n",
        "        torch.nn.utils.clip_grad_norm_(modelSigmoid.parameters(), 5)\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Update the training loss and accuracy\n",
        "        train_loss += loss.item() * images.size(0)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        train_correct += (predicted == labels).sum().item()\n",
        "        train_total += labels.size(0)\n",
        "        \n",
        "        if (i+1) % 100 == 0:\n",
        "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
        "\n",
        "    # Evaluate the model on the validation set\n",
        "    val_loss = 0.0\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images = images.reshape(-1, 28*28).to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = modelSigmoid(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            val_correct += (predicted == labels).sum().item()\n",
        "            val_total += labels.size(0)\n",
        "\n",
        "    # Compute the average training and validation loss and accuracy for the epoch\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "    val_loss /= len(val_loader.dataset)\n",
        "    \n",
        "    if train_total > 0:\n",
        "        train_acc = 100 * train_correct / train_total\n",
        "    else:\n",
        "        train_acc = 0\n",
        "\n",
        "    val_acc = 100 * val_correct / val_total\n",
        "\n",
        "    # Print the training and validation loss and accuracy for the epoch\n",
        "    print (f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%')\n",
        "\n",
        "    # Record the training and validation loss and accuracy for the epoch\n",
        "    train_loss_history.append(train_loss)\n",
        "    val_loss_history.append"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_uEt9af8sZAV",
        "outputId": "535982b2-999f-4249-cf0b-fa5e46ec6172"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the 10000 test images: 97.68 %\n"
          ]
        }
      ],
      "source": [
        "# Test the model: we don't need to compute gradients\n",
        "with torch.no_grad():\n",
        "    n_correct = 0\n",
        "    n_examples = len(test_loader.dataset)\n",
        "\n",
        "    for images, labels in test_loader:\n",
        "        images = images.reshape(-1, 28*28).to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = modelSigmoid(images)\n",
        "\n",
        "        # max returns (output_value ,index)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        n_correct += (predicted == labels).sum().item()\n",
        "\n",
        "    acc = n_correct / n_examples\n",
        "    print(f'Accuracy of the network on the {n_examples} test images: {100*acc} %')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5AIp0NZX6Lu",
        "outputId": "fee33916-f899-4fc1-ce05-0a20cd32ed73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/12], Step [100/500], Loss: 0.3936\n",
            "Epoch [1/12], Step [200/500], Loss: 0.3190\n",
            "Epoch [1/12], Step [300/500], Loss: 0.2793\n",
            "Epoch [1/12], Step [400/500], Loss: 0.4455\n",
            "Epoch [1/12], Step [500/500], Loss: 0.2655\n",
            "Epoch [1/12], Train Loss: 0.3489, Val Loss: 0.2579, Train Acc: 90.02%, Val Acc: 92.26%\n",
            "Epoch [2/12], Step [100/500], Loss: 0.0909\n",
            "Epoch [2/12], Step [200/500], Loss: 0.1204\n",
            "Epoch [2/12], Step [300/500], Loss: 0.1389\n",
            "Epoch [2/12], Step [400/500], Loss: 0.0587\n",
            "Epoch [2/12], Step [500/500], Loss: 0.3987\n",
            "Epoch [2/12], Train Loss: 0.1844, Val Loss: 0.1704, Train Acc: 94.76%, Val Acc: 95.03%\n",
            "Epoch [3/12], Step [100/500], Loss: 0.0539\n",
            "Epoch [3/12], Step [200/500], Loss: 0.1732\n",
            "Epoch [3/12], Step [300/500], Loss: 0.1107\n",
            "Epoch [3/12], Step [400/500], Loss: 0.2103\n",
            "Epoch [3/12], Step [500/500], Loss: 0.0464\n",
            "Epoch [3/12], Train Loss: 0.1251, Val Loss: 0.1358, Train Acc: 96.33%, Val Acc: 95.81%\n",
            "Epoch [4/12], Step [100/500], Loss: 0.0964\n",
            "Epoch [4/12], Step [200/500], Loss: 0.0412\n",
            "Epoch [4/12], Step [300/500], Loss: 0.0614\n",
            "Epoch [4/12], Step [400/500], Loss: 0.0890\n",
            "Epoch [4/12], Step [500/500], Loss: 0.0654\n",
            "Epoch [4/12], Train Loss: 0.0931, Val Loss: 0.1134, Train Acc: 97.26%, Val Acc: 96.61%\n",
            "Epoch [5/12], Step [100/500], Loss: 0.0297\n",
            "Epoch [5/12], Step [200/500], Loss: 0.0438\n",
            "Epoch [5/12], Step [300/500], Loss: 0.1172\n",
            "Epoch [5/12], Step [400/500], Loss: 0.0470\n",
            "Epoch [5/12], Step [500/500], Loss: 0.1198\n",
            "Epoch [5/12], Train Loss: 0.0695, Val Loss: 0.1081, Train Acc: 97.95%, Val Acc: 96.66%\n",
            "Epoch [6/12], Step [100/500], Loss: 0.0276\n",
            "Epoch [6/12], Step [200/500], Loss: 0.0356\n",
            "Epoch [6/12], Step [300/500], Loss: 0.0292\n",
            "Epoch [6/12], Step [400/500], Loss: 0.0411\n",
            "Epoch [6/12], Step [500/500], Loss: 0.1156\n",
            "Epoch [6/12], Train Loss: 0.0527, Val Loss: 0.1003, Train Acc: 98.50%, Val Acc: 96.94%\n",
            "Epoch [7/12], Step [100/500], Loss: 0.1001\n",
            "Epoch [7/12], Step [200/500], Loss: 0.0753\n",
            "Epoch [7/12], Step [300/500], Loss: 0.0307\n",
            "Epoch [7/12], Step [400/500], Loss: 0.0269\n",
            "Epoch [7/12], Step [500/500], Loss: 0.0243\n",
            "Epoch [7/12], Train Loss: 0.0402, Val Loss: 0.0893, Train Acc: 98.87%, Val Acc: 97.31%\n",
            "Epoch [8/12], Step [100/500], Loss: 0.0188\n",
            "Epoch [8/12], Step [200/500], Loss: 0.0121\n",
            "Epoch [8/12], Step [300/500], Loss: 0.0226\n",
            "Epoch [8/12], Step [400/500], Loss: 0.0235\n",
            "Epoch [8/12], Step [500/500], Loss: 0.0531\n",
            "Epoch [8/12], Train Loss: 0.0315, Val Loss: 0.0882, Train Acc: 99.15%, Val Acc: 97.25%\n",
            "Epoch [9/12], Step [100/500], Loss: 0.0178\n",
            "Epoch [9/12], Step [200/500], Loss: 0.0154\n",
            "Epoch [9/12], Step [300/500], Loss: 0.0625\n",
            "Epoch [9/12], Step [400/500], Loss: 0.0125\n",
            "Epoch [9/12], Step [500/500], Loss: 0.0137\n",
            "Epoch [9/12], Train Loss: 0.0223, Val Loss: 0.0914, Train Acc: 99.46%, Val Acc: 97.32%\n",
            "Epoch [10/12], Step [100/500], Loss: 0.0085\n",
            "Epoch [10/12], Step [200/500], Loss: 0.0071\n",
            "Epoch [10/12], Step [300/500], Loss: 0.0179\n",
            "Epoch [10/12], Step [400/500], Loss: 0.0158\n",
            "Epoch [10/12], Step [500/500], Loss: 0.0376\n",
            "Epoch [10/12], Train Loss: 0.0177, Val Loss: 0.0877, Train Acc: 99.58%, Val Acc: 97.37%\n",
            "Epoch [11/12], Step [100/500], Loss: 0.0056\n",
            "Epoch [11/12], Step [200/500], Loss: 0.0165\n",
            "Epoch [11/12], Step [300/500], Loss: 0.0317\n",
            "Epoch [11/12], Step [400/500], Loss: 0.0126\n",
            "Epoch [11/12], Step [500/500], Loss: 0.0065\n",
            "Epoch [11/12], Train Loss: 0.0134, Val Loss: 0.0912, Train Acc: 99.71%, Val Acc: 97.47%\n",
            "Epoch [12/12], Step [100/500], Loss: 0.0073\n",
            "Epoch [12/12], Step [200/500], Loss: 0.0141\n",
            "Epoch [12/12], Step [300/500], Loss: 0.0062\n",
            "Epoch [12/12], Step [400/500], Loss: 0.0154\n",
            "Epoch [12/12], Step [500/500], Loss: 0.0114\n",
            "Epoch [12/12], Train Loss: 0.0099, Val Loss: 0.0852, Train Acc: 99.81%, Val Acc: 97.60%\n"
          ]
        }
      ],
      "source": [
        "class NeuralNetworkTanh(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, num_classes):\n",
        "    super(NeuralNetworkTanh, self).__init__()\n",
        "    self.l1 = nn.Linear(input_size, hidden_size) \n",
        "    self.tanh = nn.Tanh()\n",
        "    self.l2 = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.l1(x)\n",
        "    out = self.tanh(out)\n",
        "    out = self.l2(out)\n",
        "    return out\n",
        "\n",
        "modelTanh = NeuralNetworkTanh(input_size, hidden_size, num_classes).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = torch.optim.Adam(modelTanh.parameters(), lr=learning_rate)\n",
        "\n",
        "# Track the training and validation loss and accuracy for each epoch\n",
        "train_loss_history = []\n",
        "val_loss_history = []\n",
        "train_acc_history = []\n",
        "val_acc_history = []\n",
        "\n",
        "n_total_steps = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = 0.0\n",
        "    train_correct = 0\n",
        "    train_total = 0\n",
        "    for i, (images, labels) in enumerate(train_loader):  \n",
        "        # origin shape: [100, 1, 28, 28]\n",
        "        # resized: [100, 784]\n",
        "        images = images.reshape(-1, 28*28).to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        # Forward pass and loss calculation\n",
        "        outputs = modelTanh(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Backward and optimize\n",
        "        loss.backward()\n",
        "        # clip gradients \n",
        "        torch.nn.utils.clip_grad_norm_(modelTanh.parameters(), 5)\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Update the training loss and accuracy\n",
        "        train_loss += loss.item() * images.size(0)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        train_correct += (predicted == labels).sum().item()\n",
        "        train_total += labels.size(0)\n",
        "        \n",
        "        if (i+1) % 100 == 0:\n",
        "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
        "\n",
        "    # Evaluate the model on the validation set\n",
        "    val_loss = 0.0\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images = images.reshape(-1, 28*28).to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = modelTanh(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            val_correct += (predicted == labels).sum().item()\n",
        "            val_total += labels.size(0)\n",
        "\n",
        "    # Compute the average training and validation loss and accuracy for the epoch\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "    val_loss /= len(val_loader.dataset)\n",
        "    \n",
        "    if train_total > 0:\n",
        "        train_acc = 100 * train_correct / train_total\n",
        "    else:\n",
        "        train_acc = 0\n",
        "\n",
        "    val_acc = 100 * val_correct / val_total\n",
        "\n",
        "    # Print the training and validation loss and accuracy for the epoch\n",
        "    print (f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%')\n",
        "\n",
        "    # Record the training and validation loss and accuracy for the epoch\n",
        "    train_loss_history.append(train_loss)\n",
        "    val_loss_history.append"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JhyJ1JGjutxp",
        "outputId": "45df3991-8aad-4d98-815c-cbd4937838a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the 10000 test images: 97.77 %\n"
          ]
        }
      ],
      "source": [
        "# Test the model: we don't need to compute gradients\n",
        "with torch.no_grad():\n",
        "    n_correct = 0\n",
        "    n_examples = len(test_loader.dataset)\n",
        "\n",
        "    for images, labels in test_loader:\n",
        "        images = images.reshape(-1, 28*28).to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = modelTanh(images)\n",
        "\n",
        "        # max returns (output_value ,index)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        n_correct += (predicted == labels).sum().item()\n",
        "\n",
        "    acc = n_correct / n_examples\n",
        "    print(f'Accuracy of the network on the {n_examples} test images: {100*acc} %')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cp9S82QWXbtf",
        "outputId": "3984b5ff-c25b-42fa-d85a-675866e95a36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/12], Step [100/500], Loss: 1.6814\n",
            "Epoch [1/12], Step [200/500], Loss: 1.3085\n",
            "Epoch [1/12], Step [300/500], Loss: 1.2635\n",
            "Epoch [1/12], Step [400/500], Loss: 1.1058\n",
            "Epoch [1/12], Step [500/500], Loss: 0.9718\n",
            "Epoch [1/12], Train Loss: 1.4895, Val Loss: 0.3774, Train Acc: 88.63%, Val Acc: 88.24%\n",
            "Epoch [2/12], Step [100/500], Loss: 0.9862\n",
            "Epoch [2/12], Step [200/500], Loss: 1.3098\n",
            "Epoch [2/12], Step [300/500], Loss: 0.9048\n",
            "Epoch [2/12], Step [400/500], Loss: 0.8066\n",
            "Epoch [2/12], Step [500/500], Loss: 0.7367\n",
            "Epoch [2/12], Train Loss: 0.9518, Val Loss: 0.3177, Train Acc: 90.36%, Val Acc: 90.61%\n",
            "Epoch [3/12], Step [100/500], Loss: 0.8100\n",
            "Epoch [3/12], Step [200/500], Loss: 0.7652\n",
            "Epoch [3/12], Step [300/500], Loss: 0.8474\n",
            "Epoch [3/12], Step [400/500], Loss: 0.5802\n",
            "Epoch [3/12], Step [500/500], Loss: 0.5454\n",
            "Epoch [3/12], Train Loss: 0.7156, Val Loss: 0.2523, Train Acc: 92.30%, Val Acc: 92.83%\n",
            "Epoch [4/12], Step [100/500], Loss: 0.5628\n",
            "Epoch [4/12], Step [200/500], Loss: 0.4951\n",
            "Epoch [4/12], Step [300/500], Loss: 0.5615\n",
            "Epoch [4/12], Step [400/500], Loss: 0.5480\n",
            "Epoch [4/12], Step [500/500], Loss: 0.4660\n",
            "Epoch [4/12], Train Loss: 0.5543, Val Loss: 0.2149, Train Acc: 93.96%, Val Acc: 93.89%\n",
            "Epoch [5/12], Step [100/500], Loss: 0.5268\n",
            "Epoch [5/12], Step [200/500], Loss: 0.4095\n",
            "Epoch [5/12], Step [300/500], Loss: 0.4127\n",
            "Epoch [5/12], Step [400/500], Loss: 0.4217\n",
            "Epoch [5/12], Step [500/500], Loss: 0.5908\n",
            "Epoch [5/12], Train Loss: 0.4679, Val Loss: 0.1810, Train Acc: 94.91%, Val Acc: 94.69%\n",
            "Epoch [6/12], Step [100/500], Loss: 0.4453\n",
            "Epoch [6/12], Step [200/500], Loss: 0.3888\n",
            "Epoch [6/12], Step [300/500], Loss: 0.3898\n",
            "Epoch [6/12], Step [400/500], Loss: 0.3828\n",
            "Epoch [6/12], Step [500/500], Loss: 0.3653\n",
            "Epoch [6/12], Train Loss: 0.4229, Val Loss: 0.1674, Train Acc: 95.35%, Val Acc: 95.17%\n",
            "Epoch [7/12], Step [100/500], Loss: 0.4056\n",
            "Epoch [7/12], Step [200/500], Loss: 0.4035\n",
            "Epoch [7/12], Step [300/500], Loss: 0.3654\n",
            "Epoch [7/12], Step [400/500], Loss: 0.3763\n",
            "Epoch [7/12], Step [500/500], Loss: 0.3188\n",
            "Epoch [7/12], Train Loss: 0.3902, Val Loss: 0.1728, Train Acc: 95.89%, Val Acc: 94.91%\n",
            "Epoch [8/12], Step [100/500], Loss: 0.3651\n",
            "Epoch [8/12], Step [200/500], Loss: 0.2809\n",
            "Epoch [8/12], Step [300/500], Loss: 0.3320\n",
            "Epoch [8/12], Step [400/500], Loss: 0.3733\n",
            "Epoch [8/12], Step [500/500], Loss: 0.3093\n",
            "Epoch [8/12], Train Loss: 0.3724, Val Loss: 0.1538, Train Acc: 96.09%, Val Acc: 95.47%\n",
            "Epoch [9/12], Step [100/500], Loss: 0.3656\n",
            "Epoch [9/12], Step [200/500], Loss: 0.3244\n",
            "Epoch [9/12], Step [300/500], Loss: 0.4050\n",
            "Epoch [9/12], Step [400/500], Loss: 0.3062\n",
            "Epoch [9/12], Step [500/500], Loss: 0.3116\n",
            "Epoch [9/12], Train Loss: 0.3575, Val Loss: 0.1499, Train Acc: 96.25%, Val Acc: 95.51%\n",
            "Epoch [10/12], Step [100/500], Loss: 0.3516\n",
            "Epoch [10/12], Step [200/500], Loss: 0.3367\n",
            "Epoch [10/12], Step [300/500], Loss: 0.3652\n",
            "Epoch [10/12], Step [400/500], Loss: 0.3085\n",
            "Epoch [10/12], Step [500/500], Loss: 0.4214\n",
            "Epoch [10/12], Train Loss: 0.3498, Val Loss: 0.1498, Train Acc: 96.35%, Val Acc: 95.60%\n",
            "Epoch [11/12], Step [100/500], Loss: 0.3159\n",
            "Epoch [11/12], Step [200/500], Loss: 0.3247\n",
            "Epoch [11/12], Step [300/500], Loss: 0.3238\n",
            "Epoch [11/12], Step [400/500], Loss: 0.3679\n",
            "Epoch [11/12], Step [500/500], Loss: 0.3088\n",
            "Epoch [11/12], Train Loss: 0.3397, Val Loss: 0.1515, Train Acc: 96.48%, Val Acc: 95.67%\n",
            "Epoch [12/12], Step [100/500], Loss: 0.2981\n",
            "Epoch [12/12], Step [200/500], Loss: 0.2984\n",
            "Epoch [12/12], Step [300/500], Loss: 0.3672\n",
            "Epoch [12/12], Step [400/500], Loss: 0.2977\n",
            "Epoch [12/12], Step [500/500], Loss: 0.3030\n",
            "Epoch [12/12], Train Loss: 0.3340, Val Loss: 0.1421, Train Acc: 96.58%, Val Acc: 95.62%\n"
          ]
        }
      ],
      "source": [
        "class NeuralNetworkTanh(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, num_classes):\n",
        "    super(NeuralNetworkTanh, self).__init__()\n",
        "    self.l1 = nn.Linear(input_size, hidden_size)\n",
        "    #batch normalisation\n",
        "    self.bn1 = nn.BatchNorm1d(hidden_size)\n",
        "    self.tanh = nn.Tanh()\n",
        "    self.bn2 = nn.BatchNorm1d(hidden_size)\n",
        "    self.l2 = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.l1(x)\n",
        "    out = self.bn1(out)\n",
        "    out = self.tanh(out)\n",
        "    out = self.bn2(out)\n",
        "    out = self.l2(out)\n",
        "    return out\n",
        "\n",
        "modelTanh = NeuralNetworkTanh(input_size, hidden_size, num_classes).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# weightdecay for L2 regularization\n",
        "optimizer = torch.optim.Adam(modelTanh.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
        "\n",
        "# Track the training and validation loss and accuracy for each epoch\n",
        "train_loss_history = []\n",
        "val_loss_history = []\n",
        "train_acc_history = []\n",
        "val_acc_history = []\n",
        "\n",
        "n_total_steps = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = 0.0\n",
        "    train_correct = 0\n",
        "    train_total = 0\n",
        "    for i, (images, labels) in enumerate(train_loader):  \n",
        "        # origin shape: [100, 1, 28, 28]\n",
        "        # resized: [100, 784]\n",
        "        images = images.reshape(-1, 28*28).to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        # Forward pass and loss calculation\n",
        "        outputs = modelTanh(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        # L1 regularization\n",
        "        l1_reg = torch.tensor(0.)\n",
        "        for name, param in modelTanh.named_parameters():\n",
        "            if 'weight' in name:\n",
        "                l1_reg += torch.norm(param, 1)\n",
        "        loss += l1_reg * 0.0005\n",
        "        \n",
        "        # Backward and optimize\n",
        "        loss.backward()\n",
        "        # clip gradients \n",
        "        torch.nn.utils.clip_grad_norm_(modelTanh.parameters(), 5)\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Update the training loss and accuracy\n",
        "        train_loss += loss.item() * images.size(0)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        train_correct += (predicted == labels).sum().item()\n",
        "        train_total += labels.size(0)\n",
        "        \n",
        "        if (i+1) % 100 == 0:\n",
        "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
        "\n",
        "    # Evaluate the model on the validation set\n",
        "    val_loss = 0.0\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images = images.reshape(-1, 28*28).to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = modelTanh(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            val_correct += (predicted == labels).sum().item()\n",
        "            val_total += labels.size(0)\n",
        "\n",
        "    # Compute the average training and validation loss and accuracy for the epoch\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "    val_loss /= len(val_loader.dataset)\n",
        "    \n",
        "    if train_total > 0:\n",
        "        train_acc = 100 * train_correct / train_total\n",
        "    else:\n",
        "        train_acc = 0\n",
        "\n",
        "    val_acc = 100 * val_correct / val_total\n",
        "\n",
        "    # Print the training and validation loss and accuracy for the epoch\n",
        "    print (f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%')\n",
        "\n",
        "    # Record the training and validation loss and accuracy for the epoch\n",
        "    train_loss_history.append(train_loss)\n",
        "    val_loss_history.append"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5CpXjewdtTZd",
        "outputId": "d2506a09-a437-4dda-befc-051e74c14c69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the 10000 test images: 96.33 %\n"
          ]
        }
      ],
      "source": [
        "# Test the model: we don't need to compute gradients\n",
        "with torch.no_grad():\n",
        "    n_correct = 0\n",
        "    n_examples = len(test_loader.dataset)\n",
        "\n",
        "    for images, labels in test_loader:\n",
        "        images = images.reshape(-1, 28*28).to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = modelTanh(images)\n",
        "\n",
        "        # max returns (output_value ,index)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        n_correct += (predicted == labels).sum().item()\n",
        "\n",
        "    acc = n_correct / n_examples\n",
        "    print(f'Accuracy of the network on the {n_examples} test images: {100*acc} %')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7OdpeG2o52JJ",
        "outputId": "5f53b46e-6165-4450-c2c8-b6d14e87f2e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/12], Step [100/500], Loss: 0.2169\n",
            "Epoch [1/12], Step [200/500], Loss: 0.2572\n",
            "Epoch [1/12], Step [300/500], Loss: 0.1297\n",
            "Epoch [1/12], Step [400/500], Loss: 0.2457\n",
            "Epoch [1/12], Step [500/500], Loss: 0.1730\n",
            "Epoch [1/12], Train Loss: 0.3562, Val Loss: 0.2559, Train Acc: 89.83%, Val Acc: 92.72%\n",
            "Epoch [2/12], Step [100/500], Loss: 0.2025\n",
            "Epoch [2/12], Step [200/500], Loss: 0.1819\n",
            "Epoch [2/12], Step [300/500], Loss: 0.1394\n",
            "Epoch [2/12], Step [400/500], Loss: 0.1661\n",
            "Epoch [2/12], Step [500/500], Loss: 0.1083\n",
            "Epoch [2/12], Train Loss: 0.1891, Val Loss: 0.1709, Train Acc: 94.46%, Val Acc: 94.85%\n",
            "Epoch [3/12], Step [100/500], Loss: 0.1123\n",
            "Epoch [3/12], Step [200/500], Loss: 0.1219\n",
            "Epoch [3/12], Step [300/500], Loss: 0.1191\n",
            "Epoch [3/12], Step [400/500], Loss: 0.1054\n",
            "Epoch [3/12], Step [500/500], Loss: 0.0845\n",
            "Epoch [3/12], Train Loss: 0.1265, Val Loss: 0.1390, Train Acc: 96.29%, Val Acc: 95.81%\n",
            "Epoch [4/12], Step [100/500], Loss: 0.1227\n",
            "Epoch [4/12], Step [200/500], Loss: 0.0730\n",
            "Epoch [4/12], Step [300/500], Loss: 0.0819\n",
            "Epoch [4/12], Step [400/500], Loss: 0.0924\n",
            "Epoch [4/12], Step [500/500], Loss: 0.0750\n",
            "Epoch [4/12], Train Loss: 0.0952, Val Loss: 0.1176, Train Acc: 97.15%, Val Acc: 96.33%\n",
            "Epoch [5/12], Step [100/500], Loss: 0.0978\n",
            "Epoch [5/12], Step [200/500], Loss: 0.0364\n",
            "Epoch [5/12], Step [300/500], Loss: 0.0321\n",
            "Epoch [5/12], Step [400/500], Loss: 0.0592\n",
            "Epoch [5/12], Step [500/500], Loss: 0.0505\n",
            "Epoch [5/12], Train Loss: 0.0723, Val Loss: 0.1153, Train Acc: 97.79%, Val Acc: 96.53%\n",
            "Epoch [6/12], Step [100/500], Loss: 0.0272\n",
            "Epoch [6/12], Step [200/500], Loss: 0.0442\n",
            "Epoch [6/12], Step [300/500], Loss: 0.0557\n",
            "Epoch [6/12], Step [400/500], Loss: 0.0477\n",
            "Epoch [6/12], Step [500/500], Loss: 0.0506\n",
            "Epoch [6/12], Train Loss: 0.0571, Val Loss: 0.0951, Train Acc: 98.24%, Val Acc: 97.17%\n",
            "Epoch [7/12], Step [100/500], Loss: 0.0360\n",
            "Epoch [7/12], Step [200/500], Loss: 0.0239\n",
            "Epoch [7/12], Step [300/500], Loss: 0.0222\n",
            "Epoch [7/12], Step [400/500], Loss: 0.0159\n",
            "Epoch [7/12], Step [500/500], Loss: 0.0969\n",
            "Epoch [7/12], Train Loss: 0.0450, Val Loss: 0.0899, Train Acc: 98.68%, Val Acc: 97.28%\n",
            "Epoch [8/12], Step [100/500], Loss: 0.0437\n",
            "Epoch [8/12], Step [200/500], Loss: 0.0218\n",
            "Epoch [8/12], Step [300/500], Loss: 0.0186\n",
            "Epoch [8/12], Step [400/500], Loss: 0.0099\n",
            "Epoch [8/12], Step [500/500], Loss: 0.0111\n",
            "Epoch [8/12], Train Loss: 0.0380, Val Loss: 0.1037, Train Acc: 98.81%, Val Acc: 97.19%\n",
            "Epoch [9/12], Step [100/500], Loss: 0.0196\n",
            "Epoch [9/12], Step [200/500], Loss: 0.0167\n",
            "Epoch [9/12], Step [300/500], Loss: 0.0208\n",
            "Epoch [9/12], Step [400/500], Loss: 0.0259\n",
            "Epoch [9/12], Step [500/500], Loss: 0.0583\n",
            "Epoch [9/12], Train Loss: 0.0301, Val Loss: 0.0929, Train Acc: 99.05%, Val Acc: 97.26%\n",
            "Epoch [10/12], Step [100/500], Loss: 0.0104\n",
            "Epoch [10/12], Step [200/500], Loss: 0.0080\n",
            "Epoch [10/12], Step [300/500], Loss: 0.0652\n",
            "Epoch [10/12], Step [400/500], Loss: 0.0321\n",
            "Epoch [10/12], Step [500/500], Loss: 0.0077\n",
            "Epoch [10/12], Train Loss: 0.0242, Val Loss: 0.0926, Train Acc: 99.25%, Val Acc: 97.53%\n",
            "Epoch [11/12], Step [100/500], Loss: 0.0487\n",
            "Epoch [11/12], Step [200/500], Loss: 0.0093\n",
            "Epoch [11/12], Step [300/500], Loss: 0.0375\n",
            "Epoch [11/12], Step [400/500], Loss: 0.0289\n",
            "Epoch [11/12], Step [500/500], Loss: 0.0692\n",
            "Epoch [11/12], Train Loss: 0.0191, Val Loss: 0.1028, Train Acc: 99.46%, Val Acc: 97.12%\n",
            "Epoch [12/12], Step [100/500], Loss: 0.0055\n",
            "Epoch [12/12], Step [200/500], Loss: 0.0380\n",
            "Epoch [12/12], Step [300/500], Loss: 0.0271\n",
            "Epoch [12/12], Step [400/500], Loss: 0.0267\n",
            "Epoch [12/12], Step [500/500], Loss: 0.0492\n",
            "Epoch [12/12], Train Loss: 0.0145, Val Loss: 0.0954, Train Acc: 99.62%, Val Acc: 97.40%\n"
          ]
        }
      ],
      "source": [
        "# Define a neural network with ELU activation\n",
        "class NeuralNetworkELU(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(NeuralNetworkELU, self).__init__()\n",
        "        self.l1 = nn.Linear(input_size, hidden_size)\n",
        "        self.elu = nn.ELU()\n",
        "        self.l2 = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.l1(x)\n",
        "        out = self.elu(out)\n",
        "        out = self.l2(out)\n",
        "        return out\n",
        "\n",
        "# Create an instance of the network\n",
        "modelELU = NeuralNetworkELU(input_size, hidden_size, num_classes).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = torch.optim.Adam(modelELU.parameters(), lr=learning_rate)\n",
        "\n",
        "# Track the training and validation loss and accuracy for each epoch\n",
        "train_loss_history = []\n",
        "val_loss_history = []\n",
        "train_acc_history = []\n",
        "val_acc_history = []\n",
        "\n",
        "n_total_steps = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = 0.0\n",
        "    train_correct = 0\n",
        "    train_total = 0\n",
        "    for i, (images, labels) in enumerate(train_loader):  \n",
        "        # origin shape: [100, 1, 28, 28]\n",
        "        # resized: [100, 784]\n",
        "        images = images.reshape(-1, 28*28).to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        # Forward pass and loss calculation\n",
        "        outputs = modelELU(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Backward and optimize\n",
        "        loss.backward()\n",
        "        # clip gradients \n",
        "        torch.nn.utils.clip_grad_norm_(modelELU.parameters(), 5)\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Update the training loss and accuracy\n",
        "        train_loss += loss.item() * images.size(0)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        train_correct += (predicted == labels).sum().item()\n",
        "        train_total += labels.size(0)\n",
        "        \n",
        "        if (i+1) % 100 == 0:\n",
        "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
        "\n",
        "    # Evaluate the model on the validation set\n",
        "    val_loss = 0.0\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images = images.reshape(-1, 28*28).to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = modelELU(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            val_correct += (predicted == labels).sum().item()\n",
        "            val_total += labels.size(0)\n",
        "\n",
        "    # Compute the average training and validation loss and accuracy for the epoch\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "    val_loss /= len(val_loader.dataset)\n",
        "    \n",
        "    if train_total > 0:\n",
        "        train_acc = 100 * train_correct / train_total\n",
        "    else:\n",
        "        train_acc = 0\n",
        "\n",
        "    val_acc = 100 * val_correct / val_total\n",
        "\n",
        "    # Print the training and validation loss and accuracy for the epoch\n",
        "    print (f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%')\n",
        "\n",
        "    # Record the training and validation loss and accuracy for the epoch\n",
        "    train_loss_history.append(train_loss)\n",
        "    val_loss_history.append"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBMniWHmKJt2",
        "outputId": "e48d398e-7d7c-4d2d-807c-59044907f373"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the 10000 test images: 97.49 %\n"
          ]
        }
      ],
      "source": [
        "# Test the model: we don't need to compute gradients\n",
        "with torch.no_grad():\n",
        "    n_correct = 0\n",
        "    n_examples = len(test_loader.dataset)\n",
        "\n",
        "    for images, labels in test_loader:\n",
        "        images = images.reshape(-1, 28*28).to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = modelELU(images)\n",
        "\n",
        "        # max returns (output_value ,index)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        n_correct += (predicted == labels).sum().item()\n",
        "\n",
        "    acc = n_correct / n_examples\n",
        "    print(f'Accuracy of the network on the {n_examples} test images: {100*acc} %')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pzWD2oL0LsCr"
      },
      "outputs": [],
      "source": [
        "# param_grid = [{\n",
        "#     'hidden_size': [64, 128, 256],\n",
        "#     'learning_rate': [0.001, 0.01, 0.1],\n",
        "#     'weight_decay': [1e-8, 1e-7, 1e-6],\n",
        "#     'batch_size': [32, 64, 128]\n",
        "# }]\n",
        "\n",
        "# class TorchClassifier(BaseEstimator, ClassifierMixin):\n",
        "#     def __init__(self, model, criterion, optimizer):\n",
        "#         self.model = model\n",
        "#         self.criterion = criterion\n",
        "#         self.optimizer = optimizer\n",
        "\n",
        "#     def fit(self, images, labels):\n",
        "#         batch_size = len(images) \n",
        "#         for epoch in range(num_epochs):\n",
        "#             for i in range(batch_size):\n",
        "#                 start_idx = i * batch_size\n",
        "#                 end_idx = start_idx + batch_size\n",
        "#                 batch_images = images[start_idx:end_idx].reshape(-1, 28*28).to(device)\n",
        "#                 batch_labels = labels[start_idx:end_idx].to(device)\n",
        "\n",
        "#                 # Forward pass and loss calculation\n",
        "#                 outputs = self.model(batch_images)\n",
        "#                 loss = self.criterion(outputs, batch_labels)\n",
        "#                 l1_reg = torch.tensor(0.)\n",
        "#                 for name, param in self.model.named_parameters():\n",
        "#                     if 'weight' in name:\n",
        "#                         l1_reg += torch.norm(param, 1)\n",
        "#                 loss += l1_reg * 0.00005\n",
        "\n",
        "#                 # Backward and optimize\n",
        "#                 loss.backward()\n",
        "#                 # clip gradients \n",
        "#                 torch.nn.utils.clip_grad_norm_(self.model.parameters(), 5)\n",
        "#                 self.optimizer.step()\n",
        "#                 self.optimizer.zero_grad()\n",
        "\n",
        "#                 if (i+1) % 100 == 0:\n",
        "#                     print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
        "\n",
        "\n",
        "# # Define a neural network with ELU activation\n",
        "# class NeuralNetworkELU(nn.Module):\n",
        "#     def __init__(self, input_size, hidden_size, num_classes):\n",
        "#         super(NeuralNetworkELU, self).__init__()\n",
        "#         self.l1 = nn.Linear(input_size, hidden_size)\n",
        "#         self.bn1 = nn.BatchNorm1d(hidden_size)\n",
        "#         self.elu = nn.ELU()\n",
        "#         self.bn2 = nn.BatchNorm1d(hidden_size)\n",
        "#         self.l2 = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         out = self.l1(x)\n",
        "#         out = self.bn1(out)\n",
        "#         out = self.elu(out)\n",
        "#         out = self.bn2(out)\n",
        "#         out = self.l2(out)\n",
        "#         return out\n",
        "\n",
        "# # Create an instance of the network\n",
        "# modelELU = NeuralNetworkELU(input_size, hidden_size, num_classes).to(device)\n",
        "\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# optimizer = torch.optim.Adam(modelELU.parameters(), lr=learning_rate, weight_decay=1e-8)\n",
        "\n",
        "# classifier = TorchClassifier(modelELU, criterion, optimizer)\n",
        "\n",
        "# grid_search = GridSearchCV(classifier, param_grid=param_grid, scoring='accuracy', cv=2)\n",
        "# images, labels = next(iter(train_loader))\n",
        "# grid_search.fit(images, labels)\n",
        "\n",
        "\n",
        "# print(\"Best Hyperparameters: \", grid_search.best_params_)\n",
        "# print(\"Best Score: \", grid_search.best_score_)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bVJOPXFzk8NB",
        "outputId": "a5205082-8510-4a50-8a5f-bdcc43c0f49d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/12], Step [100/500], Loss: 1.8940\n",
            "Epoch [1/12], Step [200/500], Loss: 1.6033\n",
            "Epoch [1/12], Step [300/500], Loss: 1.1439\n",
            "Epoch [1/12], Step [400/500], Loss: 1.1662\n",
            "Epoch [1/12], Step [500/500], Loss: 1.1000\n",
            "Epoch [1/12], Train Loss: 3.3135, Val Loss: 0.2945, Train Acc: 89.40%, Val Acc: 91.37%\n",
            "Epoch [2/12], Step [100/500], Loss: 1.0802\n",
            "Epoch [2/12], Step [200/500], Loss: 1.0954\n",
            "Epoch [2/12], Step [300/500], Loss: 1.1155\n",
            "Epoch [2/12], Step [400/500], Loss: 1.0262\n",
            "Epoch [2/12], Step [500/500], Loss: 0.7875\n",
            "Epoch [2/12], Train Loss: 2.0507, Val Loss: 0.2611, Train Acc: 91.03%, Val Acc: 92.66%\n",
            "Epoch [3/12], Step [100/500], Loss: 0.9673\n",
            "Epoch [3/12], Step [200/500], Loss: 0.8164\n",
            "Epoch [3/12], Step [300/500], Loss: 0.9836\n",
            "Epoch [3/12], Step [400/500], Loss: 0.8596\n",
            "Epoch [3/12], Step [500/500], Loss: 0.9090\n",
            "Epoch [3/12], Train Loss: 1.7833, Val Loss: 0.2609, Train Acc: 91.87%, Val Acc: 92.52%\n",
            "Epoch [4/12], Step [100/500], Loss: 0.8808\n",
            "Epoch [4/12], Step [200/500], Loss: 0.8861\n",
            "Epoch [4/12], Step [300/500], Loss: 0.8077\n",
            "Epoch [4/12], Step [400/500], Loss: 0.5755\n",
            "Epoch [4/12], Step [500/500], Loss: 0.7183\n",
            "Epoch [4/12], Train Loss: 1.5207, Val Loss: 0.2598, Train Acc: 92.78%, Val Acc: 92.34%\n",
            "Epoch [5/12], Step [100/500], Loss: 0.6446\n",
            "Epoch [5/12], Step [200/500], Loss: 0.5749\n",
            "Epoch [5/12], Step [300/500], Loss: 0.5551\n",
            "Epoch [5/12], Step [400/500], Loss: 0.7138\n",
            "Epoch [5/12], Step [500/500], Loss: 0.5971\n",
            "Epoch [5/12], Train Loss: 1.2981, Val Loss: 0.2332, Train Acc: 93.61%, Val Acc: 93.36%\n",
            "Epoch [6/12], Step [100/500], Loss: 0.5681\n",
            "Epoch [6/12], Step [200/500], Loss: 0.5663\n",
            "Epoch [6/12], Step [300/500], Loss: 0.4702\n",
            "Epoch [6/12], Step [400/500], Loss: 0.6071\n",
            "Epoch [6/12], Step [500/500], Loss: 0.5234\n",
            "Epoch [6/12], Train Loss: 1.1388, Val Loss: 0.1886, Train Acc: 94.30%, Val Acc: 94.80%\n",
            "Epoch [7/12], Step [100/500], Loss: 0.4384\n",
            "Epoch [7/12], Step [200/500], Loss: 0.4775\n",
            "Epoch [7/12], Step [300/500], Loss: 0.5078\n",
            "Epoch [7/12], Step [400/500], Loss: 0.4993\n",
            "Epoch [7/12], Step [500/500], Loss: 0.3948\n",
            "Epoch [7/12], Train Loss: 1.0165, Val Loss: 0.1598, Train Acc: 94.90%, Val Acc: 95.74%\n",
            "Epoch [8/12], Step [100/500], Loss: 0.5276\n",
            "Epoch [8/12], Step [200/500], Loss: 0.4446\n",
            "Epoch [8/12], Step [300/500], Loss: 0.4653\n",
            "Epoch [8/12], Step [400/500], Loss: 0.5324\n",
            "Epoch [8/12], Step [500/500], Loss: 0.3847\n",
            "Epoch [8/12], Train Loss: 0.9317, Val Loss: 0.1586, Train Acc: 95.28%, Val Acc: 95.60%\n",
            "Epoch [9/12], Step [100/500], Loss: 0.3619\n",
            "Epoch [9/12], Step [200/500], Loss: 0.5066\n",
            "Epoch [9/12], Step [300/500], Loss: 0.5437\n",
            "Epoch [9/12], Step [400/500], Loss: 0.3954\n",
            "Epoch [9/12], Step [500/500], Loss: 0.4641\n",
            "Epoch [9/12], Train Loss: 0.8665, Val Loss: 0.1578, Train Acc: 95.65%, Val Acc: 95.39%\n",
            "Epoch [10/12], Step [100/500], Loss: 0.3853\n",
            "Epoch [10/12], Step [200/500], Loss: 0.3372\n",
            "Epoch [10/12], Step [300/500], Loss: 0.3593\n",
            "Epoch [10/12], Step [400/500], Loss: 0.4196\n",
            "Epoch [10/12], Step [500/500], Loss: 0.4010\n",
            "Epoch [10/12], Train Loss: 0.8140, Val Loss: 0.1620, Train Acc: 95.88%, Val Acc: 95.64%\n",
            "Epoch [11/12], Step [100/500], Loss: 0.4153\n",
            "Epoch [11/12], Step [200/500], Loss: 0.3291\n",
            "Epoch [11/12], Step [300/500], Loss: 0.3511\n",
            "Epoch [11/12], Step [400/500], Loss: 0.3424\n",
            "Epoch [11/12], Step [500/500], Loss: 0.3740\n",
            "Epoch [11/12], Train Loss: 0.7859, Val Loss: 0.1339, Train Acc: 96.07%, Val Acc: 96.28%\n",
            "Epoch [12/12], Step [100/500], Loss: 0.3788\n",
            "Epoch [12/12], Step [200/500], Loss: 0.4048\n",
            "Epoch [12/12], Step [300/500], Loss: 0.5028\n",
            "Epoch [12/12], Step [400/500], Loss: 0.3398\n",
            "Epoch [12/12], Step [500/500], Loss: 0.3862\n",
            "Epoch [12/12], Train Loss: 0.7540, Val Loss: 0.1209, Train Acc: 96.35%, Val Acc: 96.66%\n",
            "Epoch [1/12], Step [100/500], Loss: 0.4045\n",
            "Epoch [1/12], Step [200/500], Loss: 0.3302\n",
            "Epoch [1/12], Step [300/500], Loss: 0.4337\n",
            "Epoch [1/12], Step [400/500], Loss: 0.3814\n",
            "Epoch [1/12], Step [500/500], Loss: 0.3980\n",
            "Epoch [1/12], Train Loss: 0.7434, Val Loss: 0.1241, Train Acc: 96.16%, Val Acc: 96.57%\n",
            "Epoch [2/12], Step [100/500], Loss: 0.3524\n",
            "Epoch [2/12], Step [200/500], Loss: 0.3425\n",
            "Epoch [2/12], Step [300/500], Loss: 0.3551\n",
            "Epoch [2/12], Step [400/500], Loss: 0.3357\n",
            "Epoch [2/12], Step [500/500], Loss: 0.3910\n",
            "Epoch [2/12], Train Loss: 0.7177, Val Loss: 0.1378, Train Acc: 96.35%, Val Acc: 95.99%\n",
            "Epoch [3/12], Step [100/500], Loss: 0.3607\n",
            "Epoch [3/12], Step [200/500], Loss: 0.3056\n",
            "Epoch [3/12], Step [300/500], Loss: 0.3420\n",
            "Epoch [3/12], Step [400/500], Loss: 0.3642\n",
            "Epoch [3/12], Step [500/500], Loss: 0.3425\n",
            "Epoch [3/12], Train Loss: 0.7023, Val Loss: 0.1300, Train Acc: 96.34%, Val Acc: 96.17%\n",
            "Epoch [4/12], Step [100/500], Loss: 0.2996\n",
            "Epoch [4/12], Step [200/500], Loss: 0.4426\n",
            "Epoch [4/12], Step [300/500], Loss: 0.4006\n",
            "Epoch [4/12], Step [400/500], Loss: 0.3267\n",
            "Epoch [4/12], Step [500/500], Loss: 0.3036\n",
            "Epoch [4/12], Train Loss: 0.6749, Val Loss: 0.1168, Train Acc: 96.57%, Val Acc: 96.75%\n",
            "Epoch [5/12], Step [100/500], Loss: 0.2867\n",
            "Epoch [5/12], Step [200/500], Loss: 0.3531\n",
            "Epoch [5/12], Step [300/500], Loss: 0.3163\n",
            "Epoch [5/12], Step [400/500], Loss: 0.4181\n",
            "Epoch [5/12], Step [500/500], Loss: 0.3252\n",
            "Epoch [5/12], Train Loss: 0.6632, Val Loss: 0.1085, Train Acc: 96.56%, Val Acc: 97.13%\n",
            "Epoch [6/12], Step [100/500], Loss: 0.3076\n",
            "Epoch [6/12], Step [200/500], Loss: 0.3132\n",
            "Epoch [6/12], Step [300/500], Loss: 0.3269\n",
            "Epoch [6/12], Step [400/500], Loss: 0.3971\n",
            "Epoch [6/12], Step [500/500], Loss: 0.3022\n",
            "Epoch [6/12], Train Loss: 0.6519, Val Loss: 0.1058, Train Acc: 96.72%, Val Acc: 96.99%\n",
            "Epoch [7/12], Step [100/500], Loss: 0.3783\n",
            "Epoch [7/12], Step [200/500], Loss: 0.3265\n",
            "Epoch [7/12], Step [300/500], Loss: 0.4050\n",
            "Epoch [7/12], Step [400/500], Loss: 0.3310\n",
            "Epoch [7/12], Step [500/500], Loss: 0.2886\n",
            "Epoch [7/12], Train Loss: 0.6388, Val Loss: 0.1082, Train Acc: 96.71%, Val Acc: 96.83%\n",
            "Epoch [8/12], Step [100/500], Loss: 0.3070\n",
            "Epoch [8/12], Step [200/500], Loss: 0.2921\n",
            "Epoch [8/12], Step [300/500], Loss: 0.2660\n",
            "Epoch [8/12], Step [400/500], Loss: 0.2694\n",
            "Epoch [8/12], Step [500/500], Loss: 0.3415\n",
            "Epoch [8/12], Train Loss: 0.6254, Val Loss: 0.1236, Train Acc: 96.88%, Val Acc: 96.40%\n",
            "Epoch [9/12], Step [100/500], Loss: 0.3069\n",
            "Epoch [9/12], Step [200/500], Loss: 0.2959\n",
            "Epoch [9/12], Step [300/500], Loss: 0.2982\n",
            "Epoch [9/12], Step [400/500], Loss: 0.3151\n",
            "Epoch [9/12], Step [500/500], Loss: 0.3515\n",
            "Epoch [9/12], Train Loss: 0.6227, Val Loss: 0.1187, Train Acc: 96.80%, Val Acc: 96.80%\n",
            "Epoch [10/12], Step [100/500], Loss: 0.2833\n",
            "Epoch [10/12], Step [200/500], Loss: 0.4260\n",
            "Epoch [10/12], Step [300/500], Loss: 0.2811\n",
            "Epoch [10/12], Step [400/500], Loss: 0.2867\n",
            "Epoch [10/12], Step [500/500], Loss: 0.3337\n",
            "Epoch [10/12], Train Loss: 0.6216, Val Loss: 0.0968, Train Acc: 96.80%, Val Acc: 97.18%\n",
            "Epoch [11/12], Step [100/500], Loss: 0.2901\n",
            "Epoch [11/12], Step [200/500], Loss: 0.3080\n",
            "Epoch [11/12], Step [300/500], Loss: 0.3833\n",
            "Epoch [11/12], Step [400/500], Loss: 0.3144\n",
            "Epoch [11/12], Step [500/500], Loss: 0.3122\n",
            "Epoch [11/12], Train Loss: 0.6148, Val Loss: 0.0967, Train Acc: 96.82%, Val Acc: 97.34%\n",
            "Epoch [12/12], Step [100/500], Loss: 0.3937\n",
            "Epoch [12/12], Step [200/500], Loss: 0.2655\n",
            "Epoch [12/12], Step [300/500], Loss: 0.2743\n",
            "Epoch [12/12], Step [400/500], Loss: 0.3265\n",
            "Epoch [12/12], Step [500/500], Loss: 0.2823\n",
            "Epoch [12/12], Train Loss: 0.6097, Val Loss: 0.1045, Train Acc: 96.96%, Val Acc: 97.03%\n",
            "Epoch [1/12], Step [100/500], Loss: 0.2750\n",
            "Epoch [1/12], Step [200/500], Loss: 0.2908\n",
            "Epoch [1/12], Step [300/500], Loss: 0.3237\n",
            "Epoch [1/12], Step [400/500], Loss: 0.3164\n",
            "Epoch [1/12], Step [500/500], Loss: 0.3710\n",
            "Epoch [1/12], Train Loss: 0.6153, Val Loss: 0.1095, Train Acc: 96.80%, Val Acc: 96.78%\n",
            "Epoch [2/12], Step [100/500], Loss: 0.3201\n",
            "Epoch [2/12], Step [200/500], Loss: 0.3159\n",
            "Epoch [2/12], Step [300/500], Loss: 0.2271\n",
            "Epoch [2/12], Step [400/500], Loss: 0.2713\n",
            "Epoch [2/12], Step [500/500], Loss: 0.2600\n",
            "Epoch [2/12], Train Loss: 0.6073, Val Loss: 0.0981, Train Acc: 96.82%, Val Acc: 97.01%\n",
            "Epoch [3/12], Step [100/500], Loss: 0.2840\n",
            "Epoch [3/12], Step [200/500], Loss: 0.2711\n",
            "Epoch [3/12], Step [300/500], Loss: 0.2726\n",
            "Epoch [3/12], Step [400/500], Loss: 0.3200\n",
            "Epoch [3/12], Step [500/500], Loss: 0.2283\n",
            "Epoch [3/12], Train Loss: 0.6003, Val Loss: 0.1002, Train Acc: 96.93%, Val Acc: 97.09%\n",
            "Epoch [4/12], Step [100/500], Loss: 0.3563\n",
            "Epoch [4/12], Step [200/500], Loss: 0.2786\n",
            "Epoch [4/12], Step [300/500], Loss: 0.2639\n",
            "Epoch [4/12], Step [400/500], Loss: 0.3876\n",
            "Epoch [4/12], Step [500/500], Loss: 0.3001\n",
            "Epoch [4/12], Train Loss: 0.5998, Val Loss: 0.0927, Train Acc: 97.05%, Val Acc: 97.53%\n",
            "Epoch [5/12], Step [100/500], Loss: 0.2437\n",
            "Epoch [5/12], Step [200/500], Loss: 0.3151\n",
            "Epoch [5/12], Step [300/500], Loss: 0.2891\n",
            "Epoch [5/12], Step [400/500], Loss: 0.2624\n",
            "Epoch [5/12], Step [500/500], Loss: 0.2738\n",
            "Epoch [5/12], Train Loss: 0.5957, Val Loss: 0.0964, Train Acc: 97.02%, Val Acc: 97.24%\n",
            "Epoch [6/12], Step [100/500], Loss: 0.2746\n",
            "Epoch [6/12], Step [200/500], Loss: 0.2283\n",
            "Epoch [6/12], Step [300/500], Loss: 0.3166\n",
            "Epoch [6/12], Step [400/500], Loss: 0.2554\n",
            "Epoch [6/12], Step [500/500], Loss: 0.2406\n",
            "Epoch [6/12], Train Loss: 0.5900, Val Loss: 0.1013, Train Acc: 97.06%, Val Acc: 96.92%\n",
            "Epoch [7/12], Step [100/500], Loss: 0.3002\n",
            "Epoch [7/12], Step [200/500], Loss: 0.3176\n",
            "Epoch [7/12], Step [300/500], Loss: 0.3156\n",
            "Epoch [7/12], Step [400/500], Loss: 0.2752\n",
            "Epoch [7/12], Step [500/500], Loss: 0.3719\n",
            "Epoch [7/12], Train Loss: 0.5837, Val Loss: 0.1016, Train Acc: 97.09%, Val Acc: 97.05%\n",
            "Epoch [8/12], Step [100/500], Loss: 0.3066\n",
            "Epoch [8/12], Step [200/500], Loss: 0.2821\n",
            "Epoch [8/12], Step [300/500], Loss: 0.2730\n",
            "Epoch [8/12], Step [400/500], Loss: 0.3064\n",
            "Epoch [8/12], Step [500/500], Loss: 0.2950\n",
            "Epoch [8/12], Train Loss: 0.5800, Val Loss: 0.0983, Train Acc: 97.11%, Val Acc: 97.17%\n",
            "Epoch [9/12], Step [100/500], Loss: 0.3396\n",
            "Epoch [9/12], Step [200/500], Loss: 0.2628\n",
            "Epoch [9/12], Step [300/500], Loss: 0.2977\n",
            "Epoch [9/12], Step [400/500], Loss: 0.2449\n",
            "Epoch [9/12], Step [500/500], Loss: 0.3142\n",
            "Epoch [9/12], Train Loss: 0.5797, Val Loss: 0.0905, Train Acc: 97.17%, Val Acc: 97.39%\n",
            "Epoch [10/12], Step [100/500], Loss: 0.2513\n",
            "Epoch [10/12], Step [200/500], Loss: 0.3294\n",
            "Epoch [10/12], Step [300/500], Loss: 0.3082\n",
            "Epoch [10/12], Step [400/500], Loss: 0.2634\n",
            "Epoch [10/12], Step [500/500], Loss: 0.2378\n",
            "Epoch [10/12], Train Loss: 0.5791, Val Loss: 0.0905, Train Acc: 97.09%, Val Acc: 97.43%\n",
            "Epoch [11/12], Step [100/500], Loss: 0.2911\n",
            "Epoch [11/12], Step [200/500], Loss: 0.2241\n",
            "Epoch [11/12], Step [300/500], Loss: 0.2545\n",
            "Epoch [11/12], Step [400/500], Loss: 0.2897\n",
            "Epoch [11/12], Step [500/500], Loss: 0.2706\n",
            "Epoch [11/12], Train Loss: 0.5766, Val Loss: 0.0947, Train Acc: 97.05%, Val Acc: 97.15%\n",
            "Epoch [12/12], Step [100/500], Loss: 0.2853\n",
            "Epoch [12/12], Step [200/500], Loss: 0.2368\n",
            "Epoch [12/12], Step [300/500], Loss: 0.2318\n",
            "Epoch [12/12], Step [400/500], Loss: 0.2499\n",
            "Epoch [12/12], Step [500/500], Loss: 0.3000\n",
            "Epoch [12/12], Train Loss: 0.5771, Val Loss: 0.1017, Train Acc: 97.07%, Val Acc: 96.95%\n",
            "Epoch [1/12], Step [100/500], Loss: 0.2838\n",
            "Epoch [1/12], Step [200/500], Loss: 0.3909\n",
            "Epoch [1/12], Step [300/500], Loss: 0.3205\n",
            "Epoch [1/12], Step [400/500], Loss: 0.1988\n",
            "Epoch [1/12], Step [500/500], Loss: 0.3117\n",
            "Epoch [1/12], Train Loss: 0.5809, Val Loss: 0.1020, Train Acc: 97.00%, Val Acc: 96.94%\n",
            "Epoch [2/12], Step [100/500], Loss: 0.2948\n",
            "Epoch [2/12], Step [200/500], Loss: 0.3121\n",
            "Epoch [2/12], Step [300/500], Loss: 0.3295\n",
            "Epoch [2/12], Step [400/500], Loss: 0.3178\n",
            "Epoch [2/12], Step [500/500], Loss: 0.2990\n",
            "Epoch [2/12], Train Loss: 0.5812, Val Loss: 0.0936, Train Acc: 97.06%, Val Acc: 97.19%\n",
            "Epoch [3/12], Step [100/500], Loss: 0.2899\n",
            "Epoch [3/12], Step [200/500], Loss: 0.2737\n",
            "Epoch [3/12], Step [300/500], Loss: 0.3094\n",
            "Epoch [3/12], Step [400/500], Loss: 0.2789\n",
            "Epoch [3/12], Step [500/500], Loss: 0.2777\n",
            "Epoch [3/12], Train Loss: 0.5820, Val Loss: 0.0894, Train Acc: 97.01%, Val Acc: 97.41%\n",
            "Epoch [4/12], Step [100/500], Loss: 0.3112\n",
            "Epoch [4/12], Step [200/500], Loss: 0.3171\n",
            "Epoch [4/12], Step [300/500], Loss: 0.2933\n",
            "Epoch [4/12], Step [400/500], Loss: 0.2550\n",
            "Epoch [4/12], Step [500/500], Loss: 0.4626\n",
            "Epoch [4/12], Train Loss: 0.5693, Val Loss: 0.0928, Train Acc: 97.13%, Val Acc: 97.15%\n",
            "Epoch [5/12], Step [100/500], Loss: 0.2818\n",
            "Epoch [5/12], Step [200/500], Loss: 0.2708\n",
            "Epoch [5/12], Step [300/500], Loss: 0.3036\n",
            "Epoch [5/12], Step [400/500], Loss: 0.2396\n",
            "Epoch [5/12], Step [500/500], Loss: 0.2986\n",
            "Epoch [5/12], Train Loss: 0.5674, Val Loss: 0.0957, Train Acc: 97.16%, Val Acc: 97.17%\n",
            "Epoch [6/12], Step [100/500], Loss: 0.3837\n",
            "Epoch [6/12], Step [200/500], Loss: 0.2702\n",
            "Epoch [6/12], Step [300/500], Loss: 0.2872\n",
            "Epoch [6/12], Step [400/500], Loss: 0.2828\n",
            "Epoch [6/12], Step [500/500], Loss: 0.3642\n",
            "Epoch [6/12], Train Loss: 0.5656, Val Loss: 0.0969, Train Acc: 97.13%, Val Acc: 97.14%\n",
            "Epoch [7/12], Step [100/500], Loss: 0.2410\n",
            "Epoch [7/12], Step [200/500], Loss: 0.3990\n",
            "Epoch [7/12], Step [300/500], Loss: 0.2914\n",
            "Epoch [7/12], Step [400/500], Loss: 0.2453\n",
            "Epoch [7/12], Step [500/500], Loss: 0.3340\n",
            "Epoch [7/12], Train Loss: 0.5641, Val Loss: 0.0879, Train Acc: 97.14%, Val Acc: 97.36%\n",
            "Epoch [8/12], Step [100/500], Loss: 0.2692\n",
            "Epoch [8/12], Step [200/500], Loss: 0.2735\n",
            "Epoch [8/12], Step [300/500], Loss: 0.2207\n",
            "Epoch [8/12], Step [400/500], Loss: 0.3375\n",
            "Epoch [8/12], Step [500/500], Loss: 0.2772\n",
            "Epoch [8/12], Train Loss: 0.5603, Val Loss: 0.0883, Train Acc: 97.21%, Val Acc: 97.35%\n",
            "Epoch [9/12], Step [100/500], Loss: 0.2751\n",
            "Epoch [9/12], Step [200/500], Loss: 0.3207\n",
            "Epoch [9/12], Step [300/500], Loss: 0.3325\n",
            "Epoch [9/12], Step [400/500], Loss: 0.2569\n",
            "Epoch [9/12], Step [500/500], Loss: 0.2327\n",
            "Epoch [9/12], Train Loss: 0.5648, Val Loss: 0.0928, Train Acc: 97.10%, Val Acc: 97.33%\n",
            "Epoch [10/12], Step [100/500], Loss: 0.3160\n",
            "Epoch [10/12], Step [200/500], Loss: 0.2684\n",
            "Epoch [10/12], Step [300/500], Loss: 0.2681\n",
            "Epoch [10/12], Step [400/500], Loss: 0.2709\n",
            "Epoch [10/12], Step [500/500], Loss: 0.3832\n",
            "Epoch [10/12], Train Loss: 0.5616, Val Loss: 0.1001, Train Acc: 97.12%, Val Acc: 96.83%\n",
            "Epoch [11/12], Step [100/500], Loss: 0.3853\n",
            "Epoch [11/12], Step [200/500], Loss: 0.3770\n",
            "Epoch [11/12], Step [300/500], Loss: 0.2708\n",
            "Epoch [11/12], Step [400/500], Loss: 0.2358\n",
            "Epoch [11/12], Step [500/500], Loss: 0.3058\n",
            "Epoch [11/12], Train Loss: 0.5573, Val Loss: 0.1028, Train Acc: 97.14%, Val Acc: 97.01%\n",
            "Epoch [12/12], Step [100/500], Loss: 0.2877\n",
            "Epoch [12/12], Step [200/500], Loss: 0.3688\n",
            "Epoch [12/12], Step [300/500], Loss: 0.2442\n",
            "Epoch [12/12], Step [400/500], Loss: 0.2420\n",
            "Epoch [12/12], Step [500/500], Loss: 0.2824\n",
            "Epoch [12/12], Train Loss: 0.5545, Val Loss: 0.0923, Train Acc: 97.21%, Val Acc: 97.30%\n",
            "Epoch [1/12], Step [100/500], Loss: 0.2384\n",
            "Epoch [1/12], Step [200/500], Loss: 0.2780\n",
            "Epoch [1/12], Step [300/500], Loss: 0.2164\n",
            "Epoch [1/12], Step [400/500], Loss: 0.3544\n",
            "Epoch [1/12], Step [500/500], Loss: 0.2458\n",
            "Epoch [1/12], Train Loss: 0.5622, Val Loss: 0.0859, Train Acc: 97.15%, Val Acc: 97.37%\n",
            "Epoch [2/12], Step [100/500], Loss: 0.2542\n",
            "Epoch [2/12], Step [200/500], Loss: 0.2795\n",
            "Epoch [2/12], Step [300/500], Loss: 0.2323\n",
            "Epoch [2/12], Step [400/500], Loss: 0.3316\n",
            "Epoch [2/12], Step [500/500], Loss: 0.2297\n",
            "Epoch [2/12], Train Loss: 0.5614, Val Loss: 0.0904, Train Acc: 97.07%, Val Acc: 97.37%\n",
            "Epoch [3/12], Step [100/500], Loss: 0.3630\n",
            "Epoch [3/12], Step [200/500], Loss: 0.2290\n",
            "Epoch [3/12], Step [300/500], Loss: 0.2787\n",
            "Epoch [3/12], Step [400/500], Loss: 0.2593\n",
            "Epoch [3/12], Step [500/500], Loss: 0.2622\n",
            "Epoch [3/12], Train Loss: 0.5553, Val Loss: 0.0937, Train Acc: 97.12%, Val Acc: 97.38%\n",
            "Epoch [4/12], Step [100/500], Loss: 0.2386\n",
            "Epoch [4/12], Step [200/500], Loss: 0.2658\n",
            "Epoch [4/12], Step [300/500], Loss: 0.2549\n",
            "Epoch [4/12], Step [400/500], Loss: 0.3164\n",
            "Epoch [4/12], Step [500/500], Loss: 0.2204\n",
            "Epoch [4/12], Train Loss: 0.5544, Val Loss: 0.0923, Train Acc: 97.04%, Val Acc: 97.12%\n",
            "Epoch [5/12], Step [100/500], Loss: 0.3270\n",
            "Epoch [5/12], Step [200/500], Loss: 0.2864\n",
            "Epoch [5/12], Step [300/500], Loss: 0.2345\n",
            "Epoch [5/12], Step [400/500], Loss: 0.2575\n",
            "Epoch [5/12], Step [500/500], Loss: 0.3103\n",
            "Epoch [5/12], Train Loss: 0.5518, Val Loss: 0.1064, Train Acc: 97.15%, Val Acc: 96.70%\n",
            "Epoch [6/12], Step [100/500], Loss: 0.2578\n",
            "Epoch [6/12], Step [200/500], Loss: 0.2453\n",
            "Epoch [6/12], Step [300/500], Loss: 0.2979\n",
            "Epoch [6/12], Step [400/500], Loss: 0.2254\n",
            "Epoch [6/12], Step [500/500], Loss: 0.3978\n",
            "Epoch [6/12], Train Loss: 0.5467, Val Loss: 0.0882, Train Acc: 97.24%, Val Acc: 97.36%\n",
            "Epoch [7/12], Step [100/500], Loss: 0.2427\n",
            "Epoch [7/12], Step [200/500], Loss: 0.2336\n",
            "Epoch [7/12], Step [300/500], Loss: 0.2359\n",
            "Epoch [7/12], Step [400/500], Loss: 0.4333\n",
            "Epoch [7/12], Step [500/500], Loss: 0.2612\n",
            "Epoch [7/12], Train Loss: 0.5513, Val Loss: 0.0900, Train Acc: 97.12%, Val Acc: 97.40%\n",
            "Epoch [8/12], Step [100/500], Loss: 0.2591\n",
            "Epoch [8/12], Step [200/500], Loss: 0.2374\n",
            "Epoch [8/12], Step [300/500], Loss: 0.3202\n",
            "Epoch [8/12], Step [400/500], Loss: 0.3583\n",
            "Epoch [8/12], Step [500/500], Loss: 0.2493\n",
            "Epoch [8/12], Train Loss: 0.5478, Val Loss: 0.0965, Train Acc: 97.21%, Val Acc: 97.15%\n",
            "Epoch [9/12], Step [100/500], Loss: 0.2890\n",
            "Epoch [9/12], Step [200/500], Loss: 0.2416\n",
            "Epoch [9/12], Step [300/500], Loss: 0.2720\n",
            "Epoch [9/12], Step [400/500], Loss: 0.2123\n",
            "Epoch [9/12], Step [500/500], Loss: 0.2772\n",
            "Epoch [9/12], Train Loss: 0.5459, Val Loss: 0.0978, Train Acc: 97.18%, Val Acc: 97.08%\n",
            "Epoch [10/12], Step [100/500], Loss: 0.2780\n",
            "Epoch [10/12], Step [200/500], Loss: 0.2264\n",
            "Epoch [10/12], Step [300/500], Loss: 0.3092\n",
            "Epoch [10/12], Step [400/500], Loss: 0.4010\n",
            "Epoch [10/12], Step [500/500], Loss: 0.3055\n",
            "Epoch [10/12], Train Loss: 0.5499, Val Loss: 0.1016, Train Acc: 97.25%, Val Acc: 96.99%\n",
            "Epoch [11/12], Step [100/500], Loss: 0.2695\n",
            "Epoch [11/12], Step [200/500], Loss: 0.3234\n",
            "Epoch [11/12], Step [300/500], Loss: 0.2778\n",
            "Epoch [11/12], Step [400/500], Loss: 0.2999\n",
            "Epoch [11/12], Step [500/500], Loss: 0.3043\n",
            "Epoch [11/12], Train Loss: 0.5465, Val Loss: 0.0885, Train Acc: 97.20%, Val Acc: 97.47%\n",
            "Epoch [12/12], Step [100/500], Loss: 0.2614\n",
            "Epoch [12/12], Step [200/500], Loss: 0.3121\n",
            "Epoch [12/12], Step [300/500], Loss: 0.4113\n",
            "Epoch [12/12], Step [400/500], Loss: 0.2692\n",
            "Epoch [12/12], Step [500/500], Loss: 0.2328\n",
            "Epoch [12/12], Train Loss: 0.5478, Val Loss: 0.0864, Train Acc: 97.19%, Val Acc: 97.47%\n",
            "Fold 1 score: 96.47%\n",
            "Fold 2 score: 96.53%\n",
            "Fold 3 score: 96.50%\n",
            "Fold 4 score: 96.79%\n",
            "Fold 5 score: 96.66%\n",
            "Average score: 96.59%\n"
          ]
        }
      ],
      "source": [
        "# Define a neural network with ELU activation\n",
        "class NeuralNetworkELU(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(NeuralNetworkELU, self).__init__()\n",
        "        self.l1 = nn.Linear(input_size, hidden_size)\n",
        "        #Batch normalisation\n",
        "        self.bn1 = nn.BatchNorm1d(hidden_size)\n",
        "        self.elu = nn.ELU()\n",
        "        self.bn2 = nn.BatchNorm1d(hidden_size)\n",
        "        self.l2 = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.l1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.elu(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.l2(out)\n",
        "        return out\n",
        "\n",
        "#K folds\n",
        "k = 5\n",
        "kf = KFold(n_splits=k, shuffle=True)\n",
        "eval_scores = []\n",
        "\n",
        "# Create an instance of the network\n",
        "modelELU = NeuralNetworkELU(input_size, hidden_size, num_classes).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "#L2 regularisation\n",
        "optimizer = torch.optim.Adam(modelELU.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
        "\n",
        "#Cosine annealing scheduler\n",
        "scheduler = CosineAnnealingLR(optimizer,\n",
        "                              T_max = 32, # Maximum number of iterations.\n",
        "                             eta_min = 1e-4) # Minimum learning rate.\n",
        "\n",
        "# Track the training and validation loss and accuracy for each epoch\n",
        "train_loss_history = []\n",
        "val_loss_history = []\n",
        "train_acc_history = []\n",
        "val_acc_history = []\n",
        "\n",
        "for fold, (train_idx, test_idx) in enumerate(kf.split(train)):\n",
        "  train_loader_fold = torch.utils.data.DataLoader(train, batch_size=batch_size, sampler=torch.utils.data.SubsetRandomSampler(train_idx))\n",
        "  test_loader_fold = torch.utils.data.DataLoader(train, batch_size=batch_size, sampler=torch.utils.data.SubsetRandomSampler(test_idx))\n",
        "  \n",
        "  n_total_steps = len(train_loader)\n",
        "  for epoch in range(num_epochs):\n",
        "      train_loss = 0.0\n",
        "      train_correct = 0\n",
        "      train_total = 0\n",
        "      for i, (images, labels) in enumerate(train_loader_fold):  \n",
        "          # origin shape: [100, 1, 28, 28]\n",
        "          # resized: [100, 784]\n",
        "          images = images.reshape(-1, 28*28).to(device)\n",
        "          labels = labels.to(device)\n",
        "          \n",
        "          # Forward pass and loss calculation\n",
        "          outputs = modelELU(images)\n",
        "          loss = criterion(outputs, labels)\n",
        "          #L1 regularisation\n",
        "          l1_reg = torch.tensor(0.)\n",
        "          for name, param in modelELU.named_parameters():\n",
        "              if 'weight' in name:\n",
        "                  l1_reg += torch.norm(param, 1)\n",
        "          loss += l1_reg * 0.0005\n",
        "          \n",
        "          # Backward and optimize\n",
        "          loss.backward()\n",
        "          # clip gradients \n",
        "          torch.nn.utils.clip_grad_norm_(modelELU.parameters(), 5)\n",
        "          optimizer.step()\n",
        "          optimizer.zero_grad()\n",
        "          scheduler.step()\n",
        "          \n",
        "          # Update the training loss and accuracy\n",
        "          train_loss += loss.item() * images.size(0)\n",
        "          _, predicted = torch.max(outputs.data, 1)\n",
        "          train_correct += (predicted == labels).sum().item()\n",
        "          train_total += labels.size(0)\n",
        "\n",
        "          # Update the training loss and accuracy\n",
        "          train_loss += loss.item() * images.size(0)\n",
        "          _, predicted = torch.max(outputs.data, 1)\n",
        "          train_correct += (predicted == labels).sum().item()\n",
        "          train_total += labels.size(0)\n",
        "          \n",
        "          if (i+1) % 100 == 0:\n",
        "             print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
        "\n",
        "      # Evaluate the model on the validation set\n",
        "      val_loss = 0.0\n",
        "      val_correct = 0\n",
        "      val_total = 0\n",
        "      with torch.no_grad():\n",
        "          for images, labels in val_loader:\n",
        "              images = images.reshape(-1, 28*28).to(device)\n",
        "              labels = labels.to(device)\n",
        "\n",
        "              outputs = modelELU(images)\n",
        "              loss = criterion(outputs, labels)\n",
        "\n",
        "              val_loss += loss.item() * images.size(0)\n",
        "              _, predicted = torch.max(outputs.data, 1)\n",
        "              val_correct += (predicted == labels).sum().item()\n",
        "              val_total += labels.size(0)\n",
        "\n",
        "      # Compute the average training and validation loss and accuracy for the epoch\n",
        "      train_loss /= len(train_loader.dataset)\n",
        "      val_loss /= len(val_loader.dataset)\n",
        "      \n",
        "      if train_total > 0:\n",
        "          train_acc = 100 * train_correct / train_total\n",
        "      else:\n",
        "          train_acc = 0\n",
        "\n",
        "      val_acc = 100 * val_correct / val_total\n",
        "\n",
        "      # Print the training and validation loss and accuracy for the epoch\n",
        "      print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%')\n",
        "\n",
        "      # Record the training and validation loss and accuracy for the epoch\n",
        "      train_loss_history.append(train_loss)\n",
        "      val_loss_history.append\n",
        "\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  with torch.no_grad():\n",
        "      for images, labels in test_loader_fold:\n",
        "          images = images.reshape(-1, 28*28).to(device)\n",
        "          labels = labels.to(device)\n",
        "          outputs = modelELU(images)\n",
        "          _, predicted = torch.max(outputs.data, 1)\n",
        "          total += labels.size(0)\n",
        "          correct += (predicted == labels).sum().item()\n",
        "  accuracy = 100 * correct / total\n",
        "  eval_scores.append(accuracy)\n",
        "\n",
        "for fold, score in enumerate(eval_scores):\n",
        "    print(f'Fold {fold+1} score: {score:.2f}%')\n",
        "print(f'Average score: {sum(eval_scores)/len(eval_scores):.2f}%')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}